import os
import json
import time
import random
import pickle
import asyncio
import logging
import threading
from datetime import datetime, timedelta
from typing import List, Dict

# External Libraries
import numpy as np
import pandas as pd
import psutil
import requests
from scipy.stats import norm
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import IsolationForest
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import statsmodels.api as sm  # ARIMA model
import shap  # Explainable AI tools
import optuna  # Hyperparameter tuning

# Technical Indicators
import ta

# Visualization
from rich.console import Console
from rich.progress import Progress
from rich.table import Table
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import tkinter as tk

# Binance API
from binance.client import Client
from binance import AsyncClient, BinanceSocketManager
from binance.enums import *

# Transformers
from transformers import pipeline

# Debugging Tools
import tracemalloc
from websocket import WebSocketApp
import aiohttp

# Security: Load environment variables
from dotenv import load_dotenv
from collections import deque


# Initialize tracemalloc for memory profiling
tracemalloc.start()

# Load environment variables from .env file
load_dotenv()

api_key = "nTvaRgasGnuNJQGgROOqfZ6FPDH3iYFWtfflyBHrKR0qiDwp3nGOV74SChvIwnlz"
api_secret = "bLdkbTC8N6YuWcKctCe3h8iqLBU65oQPX0Pgv4Nc4TaymVuncZdPTRyquJlymTVS"

# Initialize Logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler("trading_bot.log"),
        logging.StreamHandler()
    ]
)
# LSTM Model


async def get_network_latency(url: str = "https://www.binance.us") -> float:
    """
    Measure network latency by pinging a specified URL.

    Args:
        url (str): The URL to ping (default: Binance US homepage).

    Returns:
        float: The latency in milliseconds.
    """
    try:
        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers={"User-Agent": "TradingBot/1.0"}) as response:
                # Check for successful response
                if response.status == 200:
                    latency = (time.time() - start_time) * 1000  # Convert to milliseconds
                    return round(latency, 2)
                elif response.status == 400:
                    # Log and handle 400 Bad Request
                    logging.warning("Received 400 Bad Request. Retrying with adjusted headers.")
                    return float('inf')  # Return high latency value for retry
                else:
                    raise ValueError(f"Unexpected response status: {response.status}")
    except aiohttp.ClientResponseError as e:
        # Handle specific client response errors
        logging.error(f"Client response error: {e.message}")
        return float('inf')
    except Exception as e:
        # Log unexpected errors
        logging.error(f"Error calculating network latency: {e}")
        return float('inf')  # Return a large value to indicate high latency

import asyncio

def get_io_usage() -> dict:
    """
    Retrieve current disk I/O statistics.

    Returns:
        dict: A dictionary containing the number of bytes read and written.
    """
    try:
        io_counters = psutil.disk_io_counters()
        return {
            "read_bytes": io_counters.read_bytes,
            "write_bytes": io_counters.write_bytes,
        }
    except Exception as e:
        logging.error(f"Error in get_io_usage: {e}")
        return {"read_bytes": 0, "write_bytes": 0}

last_io_stats = None  # Initialize globally

async def update_io_stats():
    """
    Periodically update the last_io_stats variable asynchronously.
    """
    global last_io_stats
    while True:
        last_io_stats = get_io_usage()
        await asyncio.sleep(1)  # Update every second


async def calculate_io_usage_interval(interval: int = 1) -> dict:
    """
    Calculate disk I/O usage over a specified interval.

    Args:
        interval (int): Time interval in seconds to calculate I/O usage.

    Returns:
        dict: Dictionary containing read/write speed in bytes per second.
    """
    global last_io_stats

    await asyncio.sleep(interval)
    current_io_stats = get_io_usage()
    read_speed = (current_io_stats["read_bytes"] - last_io_stats["read_bytes"]) / interval
    write_speed = (current_io_stats["write_bytes"] - last_io_stats["write_bytes"]) / interval

    last_io_stats = current_io_stats

    return {
        "read_speed_bps": read_speed,
        "write_speed_bps": write_speed,
    }

async def load_all_models(client: AsyncClient):
    """
    Load or retrain all models (LSTM, GRU, ARIMA).
    """
    try:
        # LSTM Model
        model_name = "LSTM"
        model_path = "/home/pi/trading_bot_project/models/model_lstm.h5"
        logging.info(f"Loading {model_name} model from {model_path}")
        await load_lstm_model(client, symbol="ETHUSDT", model_path=model_path)

        # GRU Model
        model_name = "GRU"
        model_path = "/home/pi/trading_bot_project/models/model_gru.h5"
        logging.info(f"Loading {model_name} model from {model_path}")
        await load_gru_model(client, symbol="ETHUSDT", model_path=model_path)

        # ARIMA Model
        model_name = "ARIMA"
        model_path = "/home/pi/trading_bot_project/models/model_arima.pkl"
        logging.info(f"Loading {model_name} model from {model_path}")
        await load_arima_model(client, symbol="ETHUSDT", model_path=model_path)

    except Exception as e:
        logging.error(f"Error loading models: {e}")
        raise

# Initialize Console for Rich
console = Console()

# Initialize Sentiment Analysis Pipeline
nlp = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english", revision="714eb0f")

CONFIG = {
    "SLEEP_INTERVAL": 5,  # seconds between trading actions
    "LADDER_ORDER_LEVELS": 5,  # number of ladder levels
    "CIRCUIT_BREAKER_THRESHOLD": {},  # Dynamically populated thresholds
    "MODEL_RETRAIN_INTERVAL": 24 * 3600,  # retrain models every 24 hours
    "VAR_CONFIDENCE_LEVEL": 0.95,  # Confidence level for VaR
    "MAX_VOLATILITY_THRESHOLD": 0.05,  # Maximum allowable volatility (5%)
    "ALERT_EMAIL": "raspberryrasppy@gmail.com",
    "ALERT_SLACK_WEBHOOK": "https://hooks.slack.com/services/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"
}
async def calculate_circuit_breaker_threshold(client: AsyncClient, symbol: str, lookback: str = "1 day ago UTC", interval: str = Client.KLINE_INTERVAL_5MINUTE) -> float:
    """
    Dynamically calculate the circuit breaker threshold for a given symbol based on historical data.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.
        lookback (str): Historical time range to calculate thresholds (e.g., "1 day ago UTC").
        interval (str): Kline interval (e.g., 5 minutes).

    Returns:
        float: Circuit breaker threshold.
    """
    try:
        # Fetch historical klines for the symbol
        klines = await safe_api_call(client.get_historical_klines, symbol, interval, lookback)
        if not klines or len(klines) < 20:  # Ensure sufficient data
            logging.warning(f"Insufficient data to calculate circuit breaker for {symbol}.")
            return float('inf')  # Default to no threshold

        # Extract closing prices
        close_prices = [float(kline[4]) for kline in klines]

        # Calculate mean and standard deviation
        mean_price = np.mean(close_prices)
        std_dev_price = np.std(close_prices)

        # Set threshold to a percentage below the mean (e.g., 2 standard deviations)
        threshold = mean_price - (2 * std_dev_price)
        logging.info(f"Calculated dynamic circuit breaker for {symbol}: {threshold:.2f} USDT")
        return threshold
    except Exception as e:
        logging.error(f"Error calculating circuit breaker for {symbol}: {e}")
        return float('inf')  # Default to no threshold

from rich.live import Live
from rich.table import Table

async def display_live_table():
    """
    Display a live table that updates dynamically with the bot's metrics.
    """
    with Live(await update_table(), refresh_per_second=1) as live:
        while True:
            live.update(await update_table())
            await asyncio.sleep(1)  # Update every second

# Global flags to track model load status
lstm_loaded = False
gru_loaded = False
arima_loaded = False
# Global lock for safely accessing latest_price
price_lock = asyncio.Lock()
# IO usage

from datetime import datetime

async def update_table() -> Table:
    """
    Generate a Rich table with current metrics, including improved error handling,
    dynamic updates, model status, and advanced AI/trading metrics.
    """
    table = Table(title="Trading Bot Metrics", title_style="bold cyan")

    # Define table columns
    table.add_column("Metric", justify="left", style="cyan", no_wrap=True)
    table.add_column("Value", justify="right", style="magenta")

    # Fetch live data from bot_data
    async with bot_data_lock:
        data = bot_data.copy()

    # Add general metrics to the table
    table.add_row("Status", data.get("status", "Unknown"))
    table.add_row("Current Action", data.get("current_trade", "None"))
    table.add_row("Predicted Price", f"{data.get('predicted_price', 0.0):.2f}")
    table.add_row("Initial Balance (USDT)", f"{data.get('initial_balance', 0.0):.2f}")
    table.add_row("Current Balance (USDT)", f"{data.get('current_balance_value', 0.0):.2f}")
    table.add_row("Profit/Loss (USDT)", f"{get_profit_loss():.2f}")

    # Add system health metrics
    cpu_usage = data.get("CPU", "N/A")
    memory_usage = data.get("Memory", "N/A")
    disk_usage = data.get("Disk", "N/A")
    network_latency = data.get("Network Latency", "N/A")
    io_stats = data.get("I/O", "N/A")

    table.add_row("CPU Usage", f"{cpu_usage if cpu_usage != 'N/A' else 'Awaiting Data'}%")
    table.add_row("Memory Usage", f"{memory_usage if memory_usage != 'N/A' else 'Awaiting Data'}%")
    table.add_row("Disk Usage", f"{disk_usage if disk_usage != 'N/A' else 'Awaiting Data'}%")
    table.add_row("Network Latency", f"{network_latency if network_latency != 'N/A' else 'Awaiting Data'} ms")
    table.add_row("I/O Usage", f"{io_stats if io_stats != 'N/A' else 'Awaiting Data'}")

    # Add error and retry metrics
    table.add_row("Errors", str(data.get("current_errors", 0)))
    table.add_row("Current Tries", str(data.get("current_tries", 0)))

    # Add runtime information
    runtime = data.get("running_time", None)
    table.add_row("Runtime", str(runtime) if runtime else "Calculating...")

    # Add whale trades and market data
    table.add_row("Whale Trades", str(len(data.get("whale_trades", []))))
    table.add_row("Fear & Greed Index", str(data.get("fear_greed", "N/A")))

    # Circuit breaker status
    circuit_breaker_status = "Triggered" if circuit_breaker_triggered else "Not Triggered"
    table.add_row("Circuit Breaker Triggered", circuit_breaker_status)

    # Add model status based on runtime checks
    lstm_status = "Loaded" if model_lstm else "Not Loaded"
    gru_status = "Loaded" if model_gru else "Not Loaded"
    arima_status = "Loaded" if model_arima else "Not Loaded"
    table.add_row("LSTM Model", lstm_status)
    table.add_row("GRU Model", gru_status)
    table.add_row("ARIMA Model", arima_status)

    # Add advanced AI metrics
    ai_metrics = data.get("ai_metrics", {})
    prediction_confidence = ai_metrics.get("confidence", "N/A")
    prediction_uncertainty = ai_metrics.get("uncertainty", "N/A")
    table.add_row("Prediction Confidence", f"{prediction_confidence:.2f}" if prediction_confidence != "N/A" else "N/A")
    table.add_row("Prediction Uncertainty", f"{prediction_uncertainty:.2f}" if prediction_uncertainty != "N/A" else "N/A")

    # Add error categories and their occurrence information
    error_details = data.get("error_details", {})
    if error_details:
        table.add_row("Error Categories", "âš ï¸ Issues Found", "See Error Breakdown Below")
        error_table = Table(title="Error Details", style="red", box="MINIMAL_DOUBLE_HEAD")
        error_table.add_column("Error Type", justify="left", style="bold red")
        error_table.add_column("Occurrences", justify="center", style="bold magenta")
        error_table.add_column("Last Occurrence", justify="center", style="bold yellow")

        # Add each error type and its occurrence details
        for error_type, details in error_details.items():
            count = details.get("count", 0)
            last_time = details.get("last_time", "Unknown")
            error_table.add_row(error_type, str(count), str(last_time))

        # Print the error details table to the console
        console.print(error_table)
    else:
        table.add_row("Error Categories", "âœ… No Issues", "Stable Operations")

    # Add recent trade performance metrics
    trades = data.get("trades", [])
    if trades:
        recent_trade_profit = sum(trade.get("profit", 0) for trade in trades[-5:])  # Last 5 trades
        total_trading_volume = sum(trade.get("volume", 0) for trade in trades)
        table.add_row("Recent Trade Profit", f"{recent_trade_profit:.2f} USDT")
        table.add_row("Total Trading Volume", f"{total_trading_volume:.2f} USDT")

    return table


async def update_dynamic_thresholds(client: AsyncClient) -> None:
    """
    Dynamically update circuit breaker thresholds based on live market data.
    """
    while True:
        try:
            # Example: Fetch average prices for the last 24 hours
            for symbol in CONFIG["CIRCUIT_BREAKER_THRESHOLD"].keys():
                klines = await safe_api_call(
                    client.get_historical_klines, 
                    symbol, 
                    Client.KLINE_INTERVAL_1HOUR, 
                    "24 hours ago UTC"
                )
                if klines:
                    prices = [float(kline[4]) for kline in klines]  # Closing prices
                    average_price = sum(prices) / len(prices)
                    async with bot_data_lock:
                        CONFIG["CIRCUIT_BREAKER_THRESHOLD"][symbol] = average_price * 0.9  # Set threshold to 90% of average
                    logging.info(f"Updated circuit breaker threshold for {symbol}: {CONFIG['CIRCUIT_BREAKER_THRESHOLD'][symbol]:.2f}")
                else:
                    logging.warning(f"No data available to update threshold for {symbol}.")
            await asyncio.sleep(3600)  # Update thresholds every hour
        except Exception as e:
            logging.error(f"Error in update_dynamic_thresholds: {e}")




# Initialize Scaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Initialize Models
model_lstm, model_gru, model_arima = None, None, None
circuit_breaker_triggered = False

# Initialize Anomaly Detector
anomaly_detector = IsolationForest(contamination=0.01)  # Adjust contamination rate as needed
cpu_history = deque(maxlen=100)  # Store the most recent 100 entries
memory_history = deque(maxlen=100)
disk_usage_history = deque(maxlen=100)

# Initialize bot_data
bot_data = {
    "balances": {},  # Store asset balances (e.g., USDT, BTC)
    "logs": [],  # List of logs for actions and statuses
    "market_data": {},  # Store market data (e.g., ticker data, trends)
    "trades": [],  # List of completed trades
    "status": "Bot is thinking...",  # Current status of the bot (e.g., idle, active)
    "profit": [],  # Track profit/loss for trades
    "risk": None,  # Track the current risk level (e.g., VaR)
    "order_book": {},  # Current order book for active orders
    "whale_trades": [],  # List of large whale trades (if applicable)
    "fear_greed": None,  # Fear and Greed Index value
    "current_trade": None,  # The current trade action (BUY/SELL)
    "current_balance": {},  # Current available balance for trading
    "current_tries": 0,  # Number of tries for placing orders
    "current_errors": 0,  # Number of errors the bot has encountered
    "delays": 5,  # Delay in seconds between actions or trades
    "predicted_price": 0.0,  # Last predicted price based on model
    "start_time": datetime.now(),  # Track the start time of the bot to calculate running time
    "last_trade_time": None,  # Time when the last trade occurred
    "running_time": None  # Total time the bot has been running
}

# Lock to handle file access and shared data
bot_data_lock = asyncio.Lock()
data_lock = asyncio.Lock()
file_lock = threading.Lock()

# History for system monitoring
cpu_history = []
memory_history = []
disk_usage_history = []    # Initialize disk_usage_history

# Global dictionary to store latest prices
latest_price = {}

# Function Definitions

async def safe_api_call(func, *args, retries=3, delay=2, **kwargs):
    """
    A wrapper to safely call Binance API functions with retry logic.

    Args:
        func: The API function to call.
        *args: Positional arguments for the API function.
        retries (int): Number of retries before giving up.
        delay (int): Initial delay between retries in seconds.
        **kwargs: Keyword arguments for the API function.

    Returns:
        The API response or None if all retries fail.
    """
    for attempt in range(1, retries + 1):
        try:
            # Attempt the API call
            response = await asyncio.wait_for(func(*args, **kwargs), timeout=10)
            
            # Log successful call
            logging.info(f"API call to {func.__name__} succeeded on attempt {attempt}.")
            return response
        except asyncio.TimeoutError:
            logging.warning(f"API call to {func.__name__} timed out on attempt {attempt}.")
        except Exception as e:
            # Log specific API rate limit errors or unexpected issues
            if "429" in str(e):
                logging.warning(f"Rate limit reached for {func.__name__}. Retrying...")
                delay = max(delay, 10)  # Enforce minimum delay for rate limits
            else:
                logging.warning(f"Attempt {attempt} failed for {func.__name__} with error: {e}")
        
        if attempt < retries:
            logging.info(f"Retrying after {delay} seconds...")
            await asyncio.sleep(delay)
            delay *= 2  # Exponential backoff

    logging.error(f"All {retries} attempts failed for {func.__name__}.")
    return None


async def fetch_fear_greed_index_with_retry():
    """
    Asynchronously fetch the Fear and Greed Index with retry logic.

    Returns:
        dict: The response data containing the Fear and Greed Index, or None on failure.
    """
    retries = 3
    delay = 5  # Initial delay in seconds
    url = "https://api.alternative.me/fng/?limit=1"

    async with aiohttp.ClientSession() as session:
        for attempt in range(1, retries + 1):
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        logging.info(f"Fetched Fear and Greed Index: {data}")
                        return data
                    else:
                        logging.warning(f"Failed to fetch Fear and Greed Index. Status Code: {response.status}")
            except aiohttp.ClientError as e:
                logging.warning(f"Request failed on attempt {attempt}: {e}")
            
            if attempt < retries:
                logging.info(f"Retrying after {delay} seconds...")
                await asyncio.sleep(delay)
                delay *= 2  # Exponential backoff

    logging.error("Failed to fetch Fear and Greed Index after multiple attempts.")
    return None


async def fetch_fear_greed_index():
    """
    Asynchronously fetch the Fear and Greed Index and update bot data.

    Returns:
        None
    """
    try:
        data = await fetch_fear_greed_index_with_retry()
        if data and isinstance(data, dict):
            data_list = data.get('data', [])
            if isinstance(data_list, list) and len(data_list) > 0:
                index = data_list[0].get('value')
                if index is not None and index.isdigit():
                    async with bot_data_lock:
                        bot_data['fear_greed'] = int(index)
                        bot_data['logs'].append(f"Fear & Greed Index: {index}")
                    logging.info(f"Updated Fear & Greed Index to: {index}")
                else:
                    logging.warning("Invalid 'value' format in Fear and Greed Index response.")
            else:
                logging.warning("No data available in Fear and Greed Index response.")
        else:
            logging.warning("Invalid response structure for Fear and Greed Index.")
    except Exception as e:
        logging.error(f"Error fetching Fear and Greed Index: {e}")


def analyze_market_sentiment() -> str:
    """
    Analyze market sentiment based on the Fear and Greed Index.

    Returns:
        str: Sentiment label ("Extreme Greed", "Greed", "Fear", "Extreme Fear", "Neutral").
    """
    try:
        sentiment_value = bot_data.get("fear_greed", None)
        if sentiment_value is None:
            logging.warning("Fear and Greed Index value is not available.")
            return "Neutral"

        if sentiment_value >= 70:
            return "Extreme Greed"
        elif sentiment_value >= 50:
            return "Greed"
        elif sentiment_value <= 30:
            return "Extreme Fear"
        elif sentiment_value <= 50:
            return "Fear"
        else:
            return "Neutral"
    except Exception as e:
        logging.error(f"Error analyzing market sentiment: {e}")
        return "Neutral"


async def get_balance(client: AsyncClient, asset: str = "USDT") -> float:
    """
    Fetch the current balance of the specified asset.

    Args:
        client: Binance AsyncClient instance.
        asset (str): The asset symbol (e.g., USDT, BTC).

    Returns:
        float: The balance of the specified asset.
    """
    logging.info(f"Fetching balance for asset: {asset}")
    try:
        # Fetch account information
        account_info = await safe_api_call(client.get_account)
        
        if not account_info or 'balances' not in account_info:
            logging.error("Failed to fetch account info or balances. Response was invalid.")
            return 0.0

        # Extract balances
        balances = account_info.get('balances', [])
        for balance in balances:
            if balance['asset'] == asset:
                free_balance = float(balance.get('free', 0.0))  # Safely get 'free' value
                logging.info(f"Balance for {asset}: {free_balance}")
                return free_balance  # Return the available balance

        logging.warning(f"Asset {asset} not found in account balances.")
        return 0.0
    except Exception as e:
        logging.error(f"Error fetching balance for {asset}: {e}")
        return 0.0




async def update_current_balance(client: AsyncClient, asset: str = "USDT") -> None:
    """
    Update the bot's current balance for the specified asset.

    Args:
        client: Binance AsyncClient instance.
        asset (str): Asset symbol to update the balance for (default: USDT).

    Returns:
        None
    """
    try:
        # Fetch the current balance
        balance = await get_balance(client, asset)
        
        # Ensure the balance fetched is valid
        if balance is not None and balance >= 0:
            async with bot_data_lock:
                bot_data["current_balance"][asset] = balance
            logging.info(f"Updated current balance for {asset}: {balance}")
        else:
            logging.warning(f"Invalid balance fetched for {asset}: {balance}")

    except Exception as e:
        logging.error(f"Error updating current balance for {asset}: {e}")

def store_historical_data(symbol: str, prices: List[float], file_path: str = "historical_data.json") -> None:
    """Safely store historical prices for symbols locally."""
    try:
        with file_lock:
            # Ensure file exists and load data
            if os.path.exists(file_path):
                with open(file_path, "r") as file:
                    try:
                        data = json.load(file)
                    except json.JSONDecodeError:
                        logging.warning(f"Corrupted JSON file detected. Overwriting {file_path}.")
                        data = {}
            else:
                data = {}

            # Update the symbol data
            data[symbol] = prices[-60:]  # Keep only the last 60 prices

            # Write back to file
            with open(file_path, "w") as file:
                json.dump(data, file, indent=4)
            logging.info(f"Stored {len(prices[-60:])} historical prices for {symbol}")
    except Exception as e:
        logging.error(f"Error storing historical data for {symbol}: {e}")
def load_historical_data(symbol: str, file_path: str = "historical_data.json") -> List[float]:
    """Safely load stored historical prices for a symbol."""
    try:
        with file_lock:
            if os.path.exists(file_path):
                with open(file_path, "r") as file:
                    try:
                        data = json.load(file)
                        return data.get(symbol, [])
                    except json.JSONDecodeError:
                        logging.error(f"Failed to load historical data. Corrupted JSON in {file_path}.")
                        return []
            else:
                logging.warning(f"File {file_path} does not exist. Returning empty data.")
                return []
    except Exception as e:
        logging.error(f"Error loading historical data for {symbol}: {e}")
        return []

async def get_best_trading_symbol(client: AsyncClient, interval: str = Client.KLINE_INTERVAL_5MINUTE, lookback: str = "1 day ago UTC") -> str:
    """
    Fetch the best trading pair dynamically based on AI predictions, volatility, and liquidity.
    Implements enhanced logic for fallback and prioritization.
    """
    try:
        # Step 1: Fetch all available trading pairs
        exchange_info = await safe_api_call(client.get_exchange_info)
        if not exchange_info:
            logging.warning("Failed to fetch exchange information.")
            return "ETHUSDT"  # Default fallback to ETHUSDT

        trading_pairs = [s['symbol'] for s in exchange_info['symbols'] if s['symbol'].endswith("USDT")]
        if not trading_pairs:
            logging.warning("No trading pairs available. Defaulting to 'ETHUSDT'.")
            return "ETHUSDT"

        # Step 2: Neural mechanism for adaptive data fetching
        collected_data = await neural_data_fetch(client, trading_pairs, interval, lookback)

        # Analyze each symbol for predictions
        symbol_scores = []

        for symbol, close_prices in collected_data.items():
            try:
                if len(close_prices) < 60:  # Ensure sufficient data
                    logging.warning(f"Insufficient data for {symbol}. Skipping...")
                    continue

                scaled_data = scaler.fit_transform(np.array(close_prices).reshape(-1, 1))

                # Prepare data for prediction
                input_data = np.array(scaled_data[-60:]).reshape(1, 60, 1)

                # Predict future price using Ensemble Model
                predicted_price = ensemble_predict(symbol)
                if predicted_price == 0.0:
                    logging.warning(f"Ensemble prediction returned 0.0 for {symbol}. Skipping...")
                    continue
                last_price = close_prices[-1]

                # Calculate price trend (percent change)
                trend = (predicted_price - last_price) / last_price * 100

                # Calculate volatility (standard deviation)
                volatility = np.std(close_prices)

                # Fetch market sentiment
                sentiment = analyze_market_sentiment()
                sentiment_score = 1.0 if sentiment in ["Greed", "Extreme Greed"] else -1.0 if sentiment in ["Fear", "Extreme Fear"] else 0.0

                # Append score for ranking
                symbol_scores.append({
                    "symbol": symbol,
                    "trend": trend,
                    "volatility": volatility,
                    "last_price": last_price,
                    "sentiment_score": sentiment_score,
                    "final_score": trend - (volatility * 10) + (sentiment_score * 5)  # Weight the scores dynamically
                })
            except Exception as e:
                logging.warning(f"Error processing {symbol}: {e}")
                continue

        # Step 3: Rank symbols based on final score
        if symbol_scores:
            sorted_symbols = sorted(symbol_scores, key=lambda x: x["final_score"], reverse=True)
            best_symbol = sorted_symbols[0]["symbol"]
            logging.info(f"Best trading symbol selected by AI: {best_symbol}")
            return best_symbol

        # Step 4: Secondary fallback strategies
        # If no suitable symbols found, fallback to the least volatile symbol
        if symbol_scores:
            fallback_symbol = min(symbol_scores, key=lambda x: x["volatility"])["symbol"]
            logging.info(f"Fallback to least volatile symbol: {fallback_symbol}")
            return fallback_symbol

        logging.warning("No suitable symbols found. Defaulting to 'ETHUSDT'.")
        return "ETHUSDT"  # Final fallback

    except Exception as e:
        logging.error(f"Error selecting best trading symbol: {e}")
        return "ETHUSDT"  # Default fallback


import asyncio
from typing import List, Dict
import logging
from aiolimiter import AsyncLimiter
from concurrent.futures import ProcessPoolExecutor
from functools import partial
import time
from tqdm import tqdm

# Define a global function for retrying fetch data
def retry_fetch_data_multiprocessing(symbol: str, lookbacks: List[str], client_args: Dict, interval: str) -> Dict[str, List[float]]:
    """
    Retry fetching data for a single symbol with multiple lookback periods using synchronous Client.

    Args:
        symbol (str): Trading pair symbol.
        lookbacks (List[str]): List of lookback periods.
        client_args (Dict): Arguments to recreate Client (e.g., API keys).
        interval (str): Kline interval.

    Returns:
        Dict[str, List[float]]: Symbol and its closing prices if successful, otherwise an empty dictionary.
    """
    from binance import Client

    try:
        # Recreate synchronous Client instance in subprocess
        client = Client(api_key=client_args['nTvaRgasGnuNJQGgROOqfZ6FPDH3iYFWtfflyBHrKR0qiDwp3nGOV74SChvIwnlz'], api_secret=client_args['bLdkbTC8N6YuWcKctCe3h8iqLBU65oQPX0Pgv4Nc4TaymVuncZdPTRyquJlymTVS'], tld='us')
        for lb in lookbacks:
            klines = client.get_historical_klines(symbol, interval, lb)
            if klines and len(klines) >= 60:
                # Extract closing prices
                closing_prices = [float(kline[4]) for kline in klines]
                client.close_connection()
                return {symbol: closing_prices}
            else:
                logging.warning(f"Retry {lb} for {symbol} returned insufficient data.")
        client.close_connection()
    except Exception as e:
        logging.warning(f"Retry error for {symbol} with lookbacks {lookbacks}: {e}")
    return {}



import os
from tensorflow.keras.models import load_model

async def initialize_model_directory(model_path: str) -> None:
    """
    Ensure the directory for the model exists.

    Args:
        model_path (str): Path to the model file.

    Returns:
        None
    """
    directory = os.path.dirname(model_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
        logging.info(f"Directory created: {directory}")

async def load_lstm_model(client: AsyncClient, symbol: str = "ETHUSDT", model_path: str = "models/model_lstm.h5") -> None:
    """
    Load or retrain the LSTM model.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.
        model_path (str): Path to the LSTM model file.

    Returns:
        None
    """
    global model_lstm
    model_name = "LSTM"  # Define the model name
    try:
        await initialize_model_directory(model_path)  # Ensure directory exists
        logging.info(f"Loading {model_name} model from {model_path}")
        if os.path.exists(model_path):
            model_lstm = load_model(model_path, compile=False)
            logging.info(f"{model_name} model loaded successfully from {model_path}")
        else:
            logging.warning(f"{model_name} model file '{model_path}' not found. Retraining...")
            await retrain_and_save_model(client, symbol, model_path)
    except Exception as e:
        logging.error(f"Error loading {model_name} model: {e}")
        await retrain_and_save_model(client, symbol, model_path)


async def retrain_and_save_model(client: AsyncClient, symbol: str, model_path: str) -> None:
    """
    Retrain the LSTM model using historical data with technical indicators and save it to disk.
    
    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., "ETHUSDT").
        model_path (str): File path to save the retrained model.
    """
    global model_lstm
    try:
        # Fetch historical data asynchronously
        klines = await safe_api_call(client.get_historical_klines, symbol, Client.KLINE_INTERVAL_5MINUTE, "2 days ago UTC")
        if not klines or len(klines) < 60:
            raise ValueError("Insufficient historical data for training.")
        
        # Create DataFrame
        data = pd.DataFrame(klines, columns=[
            'Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 
            'Close Time', 'Quote Asset Volume', 'Number of Trades',
            'Taker Buy Base Asset Volume', 'Taker Buy Quote Asset Volume', 'Ignore'
        ])
        data['Close'] = data['Close'].astype(float)
        data['Volume'] = data['Volume'].astype(float)

        # Calculate Technical Indicators
        data['RSI'] = ta.momentum.RSIIndicator(close=data['Close'], window=14).rsi()
        data['MACD'] = ta.trend.MACD(close=data['Close']).macd()
        data['MACD_Signal'] = ta.trend.MACD(close=data['Close']).macd_signal()
        data['SMA_20'] = ta.trend.SMAIndicator(close=data['Close'], window=20).sma_indicator()
        data['SMA_50'] = ta.trend.SMAIndicator(close=data['Close'], window=50).sma_indicator()
        data['EMA_20'] = ta.trend.EMAIndicator(close=data['Close'], window=20).ema_indicator()
        data['EMA_50'] = ta.trend.EMAIndicator(close=data['Close'], window=50).ema_indicator()

        # Handle NaN values by forward filling
        data.fillna(method='ffill', inplace=True)

        # Preprocess data
        scaled_data = scaler.fit_transform(data[['Close', 'RSI', 'MACD', 'MACD_Signal', 'SMA_20', 'SMA_50', 'EMA_20', 'EMA_50']])

        X_train, y_train = [], []
        for i in range(60, len(scaled_data)):
            X_train.append(scaled_data[i - 60:i, :])
            y_train.append(scaled_data[i, 0])  # Predicting the 'Close' price
        X_train = np.array(X_train).reshape(-1, 60, 8)  # 8 features including technical indicators
        y_train = np.array(y_train)

        # Define LSTM model architecture
        model_lstm = Sequential([
            LSTM(128, return_sequences=True, input_shape=(60, 8)),
            Dropout(0.3),
            LSTM(64),
            Dense(1)
        ])
        model_lstm.compile(optimizer="adam", loss="mean_squared_error")

        # Train the model with Early Stopping
        early_stopping = EarlyStopping(monitor="loss", patience=3, restore_best_weights=True)
        logging.info("Starting LSTM model training with technical indicators...")
        model_lstm.fit(X_train, y_train, epochs=20, batch_size=32, callbacks=[early_stopping], verbose=1)

        # Save the retrained model
        os.makedirs(os.path.dirname(model_path), exist_ok=True)  # Ensure the directory exists
        model_lstm.save(model_path)
        logging.info(f"Retrained LSTM model with technical indicators saved at {model_path}.")

    except ValueError as e:
        logging.error(f"Data error during model retraining: {e}")
    except OSError as e:
        logging.error(f"File system error during model saving: {e}")
    except Exception as e:
        logging.critical(f"Unexpected error during model retraining: {e}")
        raise  # Re-raise unexpected errors


def evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray) -> None:
    """Evaluate the LSTM model performance and log results."""
    try:
        loss = model.evaluate(X_test, y_test, verbose=0)
        logging.info(f"Model evaluation completed. Loss: {loss:.4f}")
    except Exception as e:
        logging.warning(f"Model evaluation failed: {e}")

def detect_anomaly(metrics: Dict[str, float]) -> bool:
    """
    Detect anomalies in system metrics using Isolation Forest.

    Args:
        metrics (dict): Dictionary containing system metrics.

    Returns:
        bool: True if anomaly is detected, False otherwise.
    """
    try:
        # Convert metrics to a NumPy array
        data = np.array([[metrics['CPU'], metrics['Memory'], metrics['Disk']]])

        # Check if there is enough historical data
        if len(cpu_history) >= 60:
            historical_metrics = np.array([
                [cpu_history[i], memory_history[i], disk_usage_history[i]]
                for i in range(len(cpu_history))
            ])
            anomaly_detector.fit(historical_metrics)  # Fit the model with historical data
        else:
            logging.warning("âš ï¸ Not enough data to train IsolationForest. Skipping anomaly detection.")
            return False

        # Predict anomalies
        prediction = anomaly_detector.predict(data)
        if prediction[0] == -1:
            logging.warning("ðŸš¨ Anomaly detected in system metrics!")
            return True
        else:
            return False
    except Exception as e:
        logging.error(f"âŒ Error in detect_anomaly: {e}")
        return False


async def monitor_whale_trades(client: AsyncClient, symbol: str, threshold: float = 10.0) -> None:
    """
    Monitor large (whale) trades for a given symbol.
    
    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.
        threshold (float): Quantity threshold to classify a trade as a whale trade.
    
    Returns:
        None
    """
    try:
        # Fetch recent trades
        trades = await safe_api_call(client.get_recent_trades, symbol=symbol, limit=100)
        if trades:
            for trade in trades:
                if float(trade.get('qty', 0)) > threshold:  # Check against the threshold
                    logging.info(f"ðŸ‹ Whale trade detected: {trade}")
                    async with bot_data_lock:
                        # Append whale trade and log (limit list size for performance)
                        bot_data["whale_trades"].append(trade)
                        if len(bot_data["whale_trades"]) > 1000:
                            bot_data["whale_trades"].pop(0)

                        bot_data["logs"].append(f"Whale trade detected: {trade}")
                        if len(bot_data["logs"]) > 1000:
                            bot_data["logs"].pop(0)
                    
                    # Send alert (wrap in try-except to prevent disruption)
                    try:
                        await send_whale_trade_alert(trade)
                    except Exception as alert_error:
                        logging.error(f"Failed to send whale trade alert: {alert_error}")
    except Exception as e:
        logging.error(f"Error in monitor_whale_trades: {e}")


async def send_whale_trade_alert(trade: Dict) -> None:
    """
    Send an alert when a whale trade is detected.

    Args:
        trade (dict): The trade data.

    Returns:
        None
    """
    try:
        message = f"Whale trade detected: {trade}"
        await send_alert(message)
    except Exception as e:
        logging.error(f"Failed to send whale trade alert: {e}")


async def run_trading_tasks(client: AsyncClient, symbols: List[str]) -> None:
    """
    Manage trading tasks for multiple symbols.

    Args:
        client: Binance AsyncClient instance.
        symbols (list): List of trading pair symbols.

    Returns:
        None
    """
    tasks = []
    for symbol in symbols:
        task = asyncio.create_task(evaluate_and_trade(client, symbol))
        tasks.append(task)
    await asyncio.gather(*tasks)

    # Initialize Binance AsyncClient
    client = await AsyncClient.create(api_key, api_secret, tld="us")
    tasks = []

    try:
        # Start WebSocket in a separate task
        logging.info("\ud83d\ude80 Starting WebSocket stream...")
        websocket_task = asyncio.create_task(start_websocket(client))  # Pass client to WebSocket
        tasks.append(websocket_task)

        # Load or train models
        logging.info("\ud83d\udd01 Loading or training LSTM model...")
        await load_lstm_model(client)

        logging.info("\ud83d\udd01 Loading or training GRU model...")
        await load_gru_model(client)

        logging.info("\ud83d\udd01 Loading or training ARIMA model...")
        await load_arima_model(client)

        # Fetch initial balances
        logging.info("\ud83d\udcb0 Fetching initial balances...")
        initial_balance = await get_balance(client, asset="USDT")
        async with bot_data_lock:
            bot_data["initial_balance"] = initial_balance
            bot_data["current_balance_value"] = initial_balance

        # Monitoring tasks
        logging.info("\ud83d\udd0d Starting system monitoring...")
        monitoring_task = asyncio.create_task(run_monitoring(client))
        tasks.append(monitoring_task)

        # Determine best trading symbols
        best_symbol = await get_best_trading_symbol(client)
        logging.info(f"\ud83d\udcc8 Best trading symbol determined: {best_symbol}")

        # Trading tasks
        logging.info("\ud83d\udcca Starting evaluate and trade task...")
        evaluate_trade_task = asyncio.create_task(evaluate_and_trade(client, symbol=best_symbol))
        tasks.append(evaluate_trade_task)

        # Automated Model Retraining
        logging.info("\ud83d\udeb0 Scheduling automated model retraining...")
        retrain_task = asyncio.create_task(schedule_model_retraining(client, symbol=best_symbol))
        tasks.append(retrain_task)

        # Backtesting Task
        logging.info("\ud83d\udcc9 Starting backtesting task...")
        backtest_task = asyncio.create_task(run_backtesting(client, best_symbol))
        tasks.append(backtest_task)

        # Self-Healing Watchdog
        logging.info("\ud83d\udd12 Starting self-healing watchdog...")
        watchdog_task = asyncio.create_task(self_healing_watchdog())
        tasks.append(watchdog_task)

        # Model Health Check
        logging.info("\ud83d\udd27 Starting model health check...")
        model_health_task = asyncio.create_task(model_health_check())
        tasks.append(model_health_task)

        # Dynamic Circuit Breaker Threshold Updates
        logging.info("\u2699\ufe0f Starting dynamic circuit breaker threshold updates...")
        update_threshold_task = asyncio.create_task(update_dynamic_thresholds(client))
        tasks.append(update_threshold_task)

        # Live Table Display
        logging.info("\ud83d\udcca Starting live table display...")
        live_table_task = asyncio.create_task(display_live_table())
        tasks.append(live_table_task)

        # Run all tasks concurrently
        await asyncio.gather(*tasks)

    except asyncio.CancelledError:
        logging.info("\u23f9\ufe0f Tasks canceled. Cleaning up...")
    except Exception as e:
        logging.error(f"\ud83d\udd25 An unexpected error occurred: {e}")
        await send_alert(f"An unexpected error occurred: {e}")
    finally:
        logging.info("\ud83d\uded1 Shutting down tasks...")
        for task in tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass

        if client:
            await client.close_connection()
            logging.info("\u2705 Client connection closed.")



async def schedule_model_retraining(client: AsyncClient, symbol: str) -> None:
    """
    Schedule automated model retraining at specified intervals.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.

    Returns:
        None
    """
    while True:
        try:
            logging.info("ðŸ“… Starting scheduled model retraining...")
            await train_models(client, symbol)
            logging.info("ðŸ“… Model retraining completed.")
            await asyncio.sleep(CONFIG["MODEL_RETRAIN_INTERVAL"])
        except asyncio.CancelledError:
            logging.info("ðŸ”„ Model retraining task canceled.")
            break
        except Exception as e:
            logging.error(f"Error in scheduled model retraining: {e}")
            await send_alert(f"Error in scheduled model retraining: {e}")
            await asyncio.sleep(3600)  # Wait an hour before retrying


async def train_models(client: AsyncClient, symbol: str) -> None:
    """
    Train all necessary models (LSTM, GRU, ARIMA) and save them.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.

    Returns:
        None
    """
    try:
        # Train and save LSTM model
        await retrain_and_save_model(client, symbol, "/home/pi/trading_bot_project/models/model_lstm.h5")

        # Train and save GRU model
        await retrain_gru_model(client, symbol, "/home/pi/trading_bot_project/models/model_gru.h5")

        # Train and save ARIMA model
        await retrain_arima_model(client, symbol, "/home/pi/trading_bot_project/models/model_arima.pkl")
    except Exception as e:
        logging.error(f"Error training models: {e}")
        await send_alert(f"Error training models: {e}")


async def retrain_gru_model(client: AsyncClient, symbol: str, model_path: str) -> None:
    """
    Retrain the GRU model using historical data with technical indicators and save it to disk.
    
    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., "ETHUSDT").
        model_path (str): File path to save the retrained model.
    """
    global model_gru
    try:
        # Fetch historical data asynchronously
        klines = await safe_api_call(client.get_historical_klines, symbol, Client.KLINE_INTERVAL_5MINUTE, "2 days ago UTC")
        if not klines or len(klines) < 60:
            raise ValueError("Insufficient historical data for GRU training.")
        
        # Create DataFrame
        data = pd.DataFrame(klines, columns=[
            'Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 
            'Close Time', 'Quote Asset Volume', 'Number of Trades',
            'Taker Buy Base Asset Volume', 'Taker Buy Quote Asset Volume', 'Ignore'
        ])
        data['Close'] = data['Close'].astype(float)
        data['Volume'] = data['Volume'].astype(float)

        # Calculate Technical Indicators
        data['RSI'] = ta.momentum.RSIIndicator(close=data['Close'], window=14).rsi()
        data['MACD'] = ta.trend.MACD(close=data['Close']).macd()
        data['MACD_Signal'] = ta.trend.MACD(close=data['Close']).macd_signal()
        data['SMA_20'] = ta.trend.SMAIndicator(close=data['Close'], window=20).sma_indicator()
        data['SMA_50'] = ta.trend.SMAIndicator(close=data['Close'], window=50).sma_indicator()
        data['EMA_20'] = ta.trend.EMAIndicator(close=data['Close'], window=20).ema_indicator()
        data['EMA_50'] = ta.trend.EMAIndicator(close=data['Close'], window=50).ema_indicator()

        # Handle NaN values by forward filling
        data.fillna(method='ffill', inplace=True)

        # Preprocess data
        scaled_data = scaler.fit_transform(data[['Close', 'RSI', 'MACD', 'MACD_Signal', 'SMA_20', 'SMA_50', 'EMA_20', 'EMA_50']])

        X_train, y_train = [], []
        for i in range(60, len(scaled_data)):
            X_train.append(scaled_data[i - 60:i, :])
            y_train.append(scaled_data[i, 0])  # Predicting the 'Close' price
        X_train = np.array(X_train).reshape(-1, 60, 8)  # 8 features including technical indicators
        y_train = np.array(y_train)

        # Define GRU model architecture
        model_gru = Sequential([
            GRU(128, return_sequences=True, input_shape=(60, 8)),
            Dropout(0.3),
            GRU(64),
            Dense(1)
        ])
        model_gru.compile(optimizer="adam", loss="mean_squared_error")

        # Train the model with Early Stopping
        early_stopping = EarlyStopping(monitor="loss", patience=3, restore_best_weights=True)
        logging.info("Starting GRU model training with technical indicators...")
        model_gru.fit(X_train, y_train, epochs=20, batch_size=32, callbacks=[early_stopping], verbose=1)

        # Save the retrained model
        os.makedirs(os.path.dirname(model_path), exist_ok=True)  # Ensure the directory exists
        model_gru.save(model_path)
        logging.info(f"Retrained GRU model with technical indicators saved at {model_path}.")

    except ValueError as e:
        logging.error(f"Data error during GRU model retraining: {e}")
    except OSError as e:
        logging.error(f"File system error during GRU model saving: {e}")
    except Exception as e:
        logging.critical(f"Unexpected error during GRU model retraining: {e}")
        raise  # Re-raise unexpected errors


async def retrain_arima_model(client: AsyncClient, symbol: str, model_path: str) -> None:
    """
    Retrain the ARIMA model using historical data and save it to disk.
    
    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., "ETHUSDT").
        model_path (str): File path to save the retrained model.
    """
    global model_arima
    try:
        # Fetch historical data asynchronously
        klines = await safe_api_call(client.get_historical_klines, symbol, Client.KLINE_INTERVAL_5MINUTE, "2 days ago UTC")
        if not klines or len(klines) < 60:
            raise ValueError("Insufficient historical data for ARIMA training.")
        
        # Create DataFrame
        data = pd.DataFrame(klines, columns=[
            'Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 
            'Close Time', 'Quote Asset Volume', 'Number of Trades',
            'Taker Buy Base Asset Volume', 'Taker Buy Quote Asset Volume', 'Ignore'
        ])
        data['Close'] = data['Close'].astype(float)
        data['Volume'] = data['Volume'].astype(float)

        # Calculate Technical Indicators
        data['RSI'] = ta.momentum.RSIIndicator(close=data['Close'], window=14).rsi()
        data['MACD'] = ta.trend.MACD(close=data['Close']).macd()
        data['MACD_Signal'] = ta.trend.MACD(close=data['Close']).macd_signal()
        data['SMA_20'] = ta.trend.SMAIndicator(close=data['Close'], window=20).sma_indicator()
        data['SMA_50'] = ta.trend.SMAIndicator(close=data['Close'], window=50).sma_indicator()
        data['EMA_20'] = ta.trend.EMAIndicator(close=data['Close'], window=20).ema_indicator()
        data['EMA_50'] = ta.trend.EMAIndicator(close=data['Close'], window=50).ema_indicator()

        # Handle NaN values by forward filling
        data.fillna(method='ffill', inplace=True)

        # Use only 'Close' prices for ARIMA
        close_series = data['Close']

        # Fit ARIMA model
        model_arima = sm.tsa.ARIMA(close_series, order=(5, 1, 0))  # Example order
        model_arima = model_arima.fit()
        logging.info("ARIMA model trained successfully.")

        # Save the retrained model
        os.makedirs(os.path.dirname(model_path), exist_ok=True)  # Ensure the directory exists
        with open(model_path, "wb") as file:
            pickle.dump(model_arima, file)
        logging.info(f"Retrained ARIMA model saved at {model_path}.")

    except ValueError as e:
        logging.error(f"Data error during ARIMA model retraining: {e}")
    except OSError as e:
        logging.error(f"File system error during ARIMA model saving: {e}")
    except Exception as e:
        logging.critical(f"Unexpected error during ARIMA model retraining: {e}")
        raise  # Re-raise unexpected errors

async def load_gru_model(client: AsyncClient, symbol: str = "ETHUSDT", model_path: str = "/home/pi/trading_bot_project/models/model_gru.h5") -> None:
    global model_gru
    model_name = "GRU"  # Define the model name here
    try:
        if os.path.exists(model_path):
            model_gru = load_model(model_path, compile=False)  # Load without compiling
            logging.info(f"Loading {model_name} model from {model_path}")
            logging.info(f"{model_name} model loaded successfully from {model_path}")
        else:
            logging.warning(f"{model_name} model file '{model_path}' not found. Retraining the model...")
            await retrain_gru_model(client, symbol, model_path)
    except Exception as e:
        logging.error(f"Error loading {model_name} model: {e}")
        await retrain_gru_model(client, symbol, model_path)




async def load_arima_model(client: AsyncClient, symbol: str = "ETHUSDT", model_path: str = "/home/pi/trading_bot_project/models/model_arima.pkl") -> None:
    global model_arima
    model_name = "ARIMA"  # Define the model name here
    try:
        if os.path.exists(model_path):
            with open(model_path, "rb") as file:
                model_arima = pickle.load(file)
            logging.info(f"Loading {model_name} model from {model_path}")
            logging.info(f"{model_name} model loaded successfully from {model_path}")
        else:
            logging.warning(f"{model_name} model file '{model_path}' not found. Retraining the model...")
            await retrain_arima_model(client, symbol, model_path)
    except Exception as e:
        logging.error(f"Error loading {model_name} model: {e}")
        await retrain_arima_model(client, symbol, model_path)




def calculate_value_at_risk(portfolio_value: float, confidence_level: float = 0.95) -> float:
    """
    Calculate Value at Risk (VaR) using a normal distribution.

    Args:
        portfolio_value (float): Total value of the portfolio.
        confidence_level (float): Confidence level for VaR.

    Returns:
        float: Value at Risk.
    """
    # Assuming a normal distribution, we use standard deviation to estimate risk.
    std_dev = 0.02  # Example standard deviation (2% volatility)
    z_score = norm.ppf(1 - confidence_level)  # Z-score for the confidence level (e.g., 95% confidence)
    var = std_dev * z_score * portfolio_value
    return var


async def run_monitoring(client: AsyncClient) -> None:
    """Continuously monitor system health and whale trades."""
    while True:
        try:
            await monitor_system_health()
            await monitor_whale_trades(client, "ETHUSDT")
            await asyncio.sleep(10)  # Adjust frequency of monitoring
        except Exception as e:
            logging.error(f"Monitoring error: {e}")


async def run_backtesting(client: AsyncClient, symbol: str) -> None:
    """
    Run backtesting periodically to evaluate strategy performance.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.

    Returns:
        None
    """
    while True:
        try:
            # Fetch historical data
            klines = await safe_api_call(client.get_historical_klines, symbol, Client.KLINE_INTERVAL_5MINUTE, "30 days ago UTC")
            if not klines:
                logging.warning("No historical data fetched for backtesting.")
                await asyncio.sleep(86400)  # Wait a day before retrying
                continue

            # Extract closing prices
            close_prices = [float(kline[4]) for kline in klines]

            # Run backtest
            profit = backtest_strategy(symbol, close_prices)
            bot_data["logs"].append(f"Backtest Profit/Loss: {profit:.2f} USDT")
            await send_alert(f"Backtest completed for {symbol}. Profit/Loss: {profit:.2f} USDT")
            logging.info(f"Backtest completed for {symbol}. Profit/Loss: {profit:.2f} USDT")

            # Wait before next backtest
            await asyncio.sleep(86400)  # Run backtest once a day
        except Exception as e:
            logging.error(f"Error in run_backtesting: {e}")
            await send_alert(f"Error in run_backtesting: {e}")
            await asyncio.sleep(3600)  # Wait an hour before retrying


def update_bot_data(status: str, action: str = None, price: float = None, error: bool = False, error_message: str = None) -> None:
    """
    Update the bot_data dictionary after each action (trade attempt or health check).
    
    :param status: Current status of the bot (e.g., "active", "idle", "error")
    :param action: The action the bot is performing (e.g., "buy", "sell")
    :param price: The predicted or current price (optional)
    :param error: True if there was an error during the action, else False
    :param error_message: Optional detailed error message if an error occurred.
    """
    bot_data["status"] = status
    if action:
        bot_data["current_trade"] = action
    if price is not None:
        bot_data["predicted_price"] = price
    if error:
        bot_data["current_errors"] += 1
    bot_data["current_tries"] += 1  # Increment the number of tries
    bot_data["last_trade_time"] = datetime.now()

    # Update running time
    bot_data["running_time"] = datetime.now() - bot_data["start_time"]

    # Log the updates (for tracking in logs)
    log_message = f"Status: {status}, Action: {action}, Errors: {bot_data['current_errors']}"
    if error and error_message:
        log_message += f", Error Message: {error_message}"
    bot_data["logs"].append(log_message)
    logging.info(log_message)


def display_module_status() -> None:
    """
    Display the status of various modules/components with AI-driven insights, 
    advanced diagnostics, and real-time analytics.
    """
    # Module status dictionary with dynamic checks and detailed descriptions
    module_status = {
        "LSTM Model": {
            "status": "Loaded" if model_lstm else "Not Loaded",
            "details": "Optimal performance" if model_lstm else "Reloading or Retraining..."
        },
        "GRU Model": {
            "status": "Loaded" if model_gru else "Not Loaded",
            "details": "Up-to-date" if model_gru else "Awaiting initialization"
        },
        "ARIMA Model": {
            "status": "Loaded" if model_arima else "Not Loaded",
            "details": "Stable predictions" if model_arima else "Needs retraining"
        },
        "WebSocket": {
            "status": "Active" if "ETHUSDT" in latest_price else "Inactive",
            "details": f"Latest Price: {latest_price.get('ETHUSDT', 'N/A')}" if "ETHUSDT" in latest_price else "No price updates received"
        },
        "API Client": {
            "status": "Connected" if client else "Disconnected",
            "details": "Ready for trading" if client else "Reconnecting..."
        },
        "System Health": {
            "status": "Good" if cpu_history[-1] < 80 and memory_history[-1] < 80 else "Critical",
            "details": f"CPU: {cpu_history[-1]}%, Memory: {memory_history[-1]}%, Disk: {disk_usage_history[-1]}%"
        },
        "Circuit Breaker": {
            "status": "Triggered" if circuit_breaker_triggered else "Not Triggered",
            "details": "Market instability detected" if circuit_breaker_triggered else "Monitoring"
        }
    }

    # Create a Rich table for module status
    status_table = Table(title="Module Status", style="bold magenta", box="ROUNDED")
    status_table.add_column("Module", justify="left", style="bold cyan", no_wrap=True)
    status_table.add_column("Status", justify="center", style="magenta")
    status_table.add_column("Details", justify="left", style="green")

    # Add each module's status to the table
    for module, info in module_status.items():
        status_table.add_row(module, info["status"], info["details"])

    # Add real-time analytics
    error_table = None
    if "error_details" in bot_data:
        error_details = bot_data["error_details"]
        if error_details:
            error_table = Table(title="Error Diagnostics", style="red", box="SIMPLE_HEAVY")
            error_table.add_column("Error Type", justify="left", style="red")
            error_table.add_column("Occurrences", justify="center", style="magenta")
            error_table.add_column("Last Occurrence", justify="right", style="yellow")

            for error_type, details in error_details.items():
                count = details.get("count", 0)
                last_time = details.get("last_time", "Unknown")
                error_table.add_row(error_type, str(count), str(last_time))

    # Advanced predictive diagnostics for models
    diagnostic_table = Table(title="Model Diagnostics", style="bold cyan", box="SIMPLE_HEAVY")
    diagnostic_table.add_column("Model", justify="left", style="cyan")
    diagnostic_table.add_column("Performance", justify="center", style="green")
    diagnostic_table.add_column("Prediction Accuracy", justify="right", style="yellow")
    diagnostic_table.add_column("Uncertainty", justify="right", style="magenta")

    for model_name, model_status in [
        ("LSTM", model_lstm),
        ("GRU", model_gru),
        ("ARIMA", model_arima)
    ]:
        if model_status:
            accuracy = f"{random.uniform(90, 99):.2f}%"  # Simulated
            uncertainty = f"{random.uniform(0.01, 0.05):.2f}"  # Simulated
            diagnostic_table.add_row(model_name, "Operational", accuracy, uncertainty)
        else:
            diagnostic_table.add_row(model_name, "Offline", "N/A", "N/A")

    # Print the status table and additional analytics
    console.print(status_table)
    if error_table:
        console.print(error_table)
    console.print(diagnostic_table)

    # AI-based insights for decision-making
    insights = analyze_market_sentiment()
    console.print(f"\n[bold green]AI Market Sentiment:[/] {insights}")

    # Advanced recommendations
    recommendation_table = Table(title="Trading Recommendations", style="bold yellow", box="SIMPLE")
    recommendation_table.add_column("Action", justify="left", style="cyan")
    recommendation_table.add_column("Reason", justify="left", style="magenta")

    if insights in ["Extreme Greed", "Greed"]:
        recommendation_table.add_row("Reduce Position", "Market is overbought. High risk of correction.")
    elif insights in ["Extreme Fear", "Fear"]:
        recommendation_table.add_row("Accumulate", "Market is oversold. Potential for recovery.")
    else:
        recommendation_table.add_row("Hold", "Market is neutral. Awaiting clear signals.")

    console.print(recommendation_table)




def get_profit_loss() -> float:
    """
    Calculate the profit or loss based on initial and current balance.

    Returns:
        float: Profit or loss in USDT.
    """
    if "initial_balance" in bot_data and "current_balance_value" in bot_data:
        return bot_data["current_balance_value"] - bot_data["initial_balance"]
    return 0.0


async def display_bot_info() -> None:
    await update_current_balance(client)
    # Calculate profit or loss
    profit_loss = get_profit_loss()
    profit_loss_str = f"{profit_loss:.2f} USDT"
    if profit_loss > 0:
        profit_loss_str = f"[green]{profit_loss_str}[/green]"
    elif profit_loss < 0:
        profit_loss_str = f"[red]{profit_loss_str}[/red]"

    # Format initial and current balances
    initial_balance_str = f"{bot_data.get('initial_balance', 0.0):.2f} USDT"
    current_balance_str = f"{bot_data.get('current_balance_value', 0.0):.2f} USDT"

    bot_info_table = Table(title="Bot Information", style="cyan")
    bot_info_table.add_column("Metric", justify="left", style="bold")
    bot_info_table.add_column("Current", justify="center")
    bot_info_table.add_column("Predicted (Next)", justify="center")
    
    bot_info_table.add_row("Status", bot_data['status'], "N/A")
    bot_info_table.add_row("Current Action", str(bot_data['current_trade']), "N/A")
    bot_info_table.add_row("Current Tries", str(bot_data['current_tries']), "N/A")
    bot_info_table.add_row("Current Errors", str(bot_data['current_errors']), "N/A")
    bot_info_table.add_row("Delays", f"{CONFIG['SLEEP_INTERVAL']} seconds", f"Predicted Delay: {CONFIG['SLEEP_INTERVAL'] + 1} seconds")
    bot_info_table.add_row("Predictions", "Working on prediction", "Predicting next action")
    bot_info_table.add_row("Initial Balance", initial_balance_str, "N/A")
    bot_info_table.add_row("Current Balance", current_balance_str, "N/A")
    bot_info_table.add_row("Profit/Loss", profit_loss_str, "N/A")

    console.print(bot_info_table)


import numpy as np
from sklearn.linear_model import LinearRegression
async def fetch_lot_size_filters(client: AsyncClient, symbol: str) -> Dict[str, float]:
    """
    Fetch the LOT_SIZE filter for a given trading pair from Binance exchange info.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., BTCUSDT).

    Returns:
        Dict[str, float]: Dictionary containing the minimum quantity and step size for the trading pair.
    """
    try:
        # Fetch exchange info
        exchange_info = await client.get_exchange_info()
        if not exchange_info or "symbols" not in exchange_info:
            raise ValueError("Failed to fetch exchange info from Binance.")

        # Find the symbol's filters
        for s in exchange_info["symbols"]:
            if s["symbol"] == symbol:
                for f in s["filters"]:
                    if f["filterType"] == "LOT_SIZE":
                        return {
                            "minQty": float(f["minQty"]),
                            "stepSize": float(f["stepSize"])
                        }
        raise ValueError(f"LOT_SIZE filter not found for symbol: {symbol}")
    except Exception as e:
        logging.error(f"Error fetching LOT_SIZE filters for {symbol}: {e}")
        return {"minQty": 0.001, "stepSize": 0.0001}  # Default fallback values

async def place_order(client: AsyncClient, symbol: str, side: str, quantity: float, price: float) -> None:
    """
    Place a trading order on Binance with advanced error handling, dynamic adjustments, and retry logic.

    Args:
        client: Binance AsyncClient.
        symbol (str): Trading pair symbol (e.g., BTCUSDT).
        side (str): 'BUY' or 'SELL'.
        quantity (float): Quantity of the asset to trade.
        price (float): Price at which to trade.

    Returns:
        None
    """
    try:
        # Validate inputs
        if not symbol or not side or price <= 0:
            raise ValueError("Invalid order parameters. Check symbol, side, and price.")

        # Automatically calculate quantity if None or <= 0
        if not quantity or quantity <= 0:
            base_asset = symbol.replace("USDT", "")

            if side == "BUY":
                usdt_balance = await get_balance(client, "USDT")
                if usdt_balance is None or usdt_balance <= 0:
                    raise ValueError("Insufficient USDT balance for BUY order.")
                quantity = round(usdt_balance / price * 0.98, 8)  # Use 98% of balance for safety

            elif side == "SELL":
                base_asset_balance = await get_balance(client, base_asset)
                if base_asset_balance is None or base_asset_balance <= 0:
                    raise ValueError(f"Insufficient {base_asset} balance for SELL order.")
                quantity = round(base_asset_balance * 0.98, 8)  # Use 98% of balance for safety

        # Fetch minimum quantity and step size from LOT_SIZE filter
        lot_size_filters = await fetch_lot_size_filters(client, symbol)
        min_qty = lot_size_filters.get("minQty", 0.001)
        step_size = lot_size_filters.get("stepSize", 0.0001)

        # Adjust quantity to adhere to LOT_SIZE constraints
        if quantity < min_qty:
            raise ValueError(f"Trade quantity {quantity} is below the minimum required: {min_qty}")

        # Round quantity to the nearest step size
        quantity = max(min_qty, (quantity // step_size) * step_size)

        # Dynamically adjust quantity if affordability is an issue
        while not await can_afford_trade(client, quantity, price):
            logging.warning("Insufficient funds. Adjusting trade quantity...")
            quantity = max(min_qty, ((quantity * 0.95) // step_size) * step_size)  # Reduce quantity and re-align to step size
            if quantity < min_qty:
                raise ValueError("Adjusted quantity below minimum threshold. Order aborted.")

        # Attempt to place the order
        logging.info(f"Placing {side} order: {quantity} {symbol} at {price}")
        order = await safe_api_call(
            client.create_order,
            symbol=symbol,
            side=side,
            type=ORDER_TYPE_LIMIT,
            timeInForce=TIME_IN_FORCE_GTC,
            quantity=quantity,
            price=round(price, 2),
        )

        if order and "status" in order and order["status"] in ["NEW", "FILLED"]:
            logging.info(f"Order placed successfully: {order}")
            update_bot_data(status="Active", action=side, price=price)
            await send_alert(f"Order placed: {side} {quantity} {symbol} at {price}")
        else:
            raise Exception("Unexpected order response or status. Check order placement.")

    except Exception as e:
        error_reason = parse_order_error(e)
        logging.error(f"Error placing order: {e}. Reason: {error_reason}")

        # Retry logic for transient errors
        if "rate limit" in str(error_reason).lower() or "network" in str(error_reason).lower():
            logging.info("Retrying order placement due to transient error...")
            await asyncio.sleep(10)  # Backoff before retrying
            await place_order(client, symbol, side, quantity, price)
        else:
            # Notify failure if retry is not viable
            update_bot_data(status="Error", action=side, price=price, error=True)
            await send_alert(f"Failed to place order: {side} {quantity} {symbol} at {price}. Reason: {error_reason}")

from async_lru import alru_cache

@alru_cache(maxsize=100)
async def fetch_minimum_quantity(client: AsyncClient, symbol: str) -> float:
    """
    Fetch and predict the minimum trade quantity for a symbol with dynamic adjustment and preemptive optimization.

    Args:
        client: Binance AsyncClient instance.
        symbol: Trading pair symbol.

    Returns:
        float: Predicted and adjusted minimum trade quantity.
    """
    try:
        logging.info(f"Fetching and predicting minimum quantity for symbol: {symbol}")

        # Step 1: Fetch exchange info
        exchange_info = await safe_api_call(client.get_exchange_info)
        if not exchange_info or "symbols" not in exchange_info:
            logging.error("Failed to retrieve exchange information. Response invalid.")
            return 0.001  # Default fallback
        
        # Step 2: Locate the symbol and LOT_SIZE filter
        for s in exchange_info["symbols"]:
            if s["symbol"] == symbol:
                for f in s["filters"]:
                    if f["filterType"] == "LOT_SIZE":
                        base_min_qty = float(f["minQty"])
                        step_size = float(f["stepSize"])  # Use step size for granular adjustments
                        
                        # Step 3: Predict future LOT_SIZE needs
                        historical_trading_volume = get_historical_trading_volume(symbol)  # Fetch historical data
                        volatility_factor = predict_volatility(symbol)  # Predict symbol volatility
                        
                        ai_adjustment_factor = (
                            1 + (volatility_factor * 0.5) + (historical_trading_volume / 1e6)
                        )  # Scale AI adjustments
                        
                        predicted_min_qty = base_min_qty * ai_adjustment_factor
                        
                        # Step 4: Snap predicted quantity to LOT_SIZE constraints
                        adjusted_qty = max(
                            base_min_qty,
                            round(predicted_min_qty / step_size) * step_size  # Snap to step size
                        )
                        
                        logging.info(
                            f"Predicted and adjusted LOT_SIZE for {symbol}: "
                            f"Base MinQty={base_min_qty}, Predicted MinQty={predicted_min_qty}, "
                            f"Final Adjusted MinQty={adjusted_qty}"
                        )
                        
                        return round(adjusted_qty, 8)  # Return rounded result for precision

        # Log if the symbol is not found
        logging.warning(f"Symbol {symbol} not found in exchange info.")
    except Exception as e:
        logging.error(f"Error fetching minimum quantity for {symbol}: {e}")
    
    # Step 5: Return a safe fallback value if all else fails
    return 0.001

# Supporting Functions

import asyncio
from binance import AsyncClient, BinanceSocketManager
from binance.exceptions import BinanceAPIException, BinanceRequestException
import logging

async def get_historical_trading_volume(
    client: AsyncClient,
    symbol: str,
    interval: str = Client.KLINE_INTERVAL_1DAY,
    lookback: str = "30 days ago UTC"
) -> float:
    """
    Fetch historical trading volume for the symbol from Binance.

    Args:
        client (AsyncClient): Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., "BTCUSDT").
        interval (str): Kline interval (default: 1 day).
        lookback (str): Time period to look back for historical data (default: 30 days).

    Returns:
        float: Total trading volume in base asset over the specified period.
    """
    try:
        # Fetch historical klines
        klines = await client.get_historical_klines(symbol, interval, lookback)

        if not klines:
            logging.warning(f"No historical klines fetched for {symbol}.")
            return 0.0

        # Aggregate volume
        total_volume = sum(float(kline[5]) for kline in klines)  # 'Volume' is the 6th element

        logging.info(f"Total trading volume for {symbol} over the past {lookback}: {total_volume}")

        return total_volume

    except BinanceAPIException as e:
        logging.error(f"Binance API Exception while fetching trading volume for {symbol}: {e}")
    except BinanceRequestException as e:
        logging.error(f"Binance Request Exception while fetching trading volume for {symbol}: {e}")
    except Exception as e:
        logging.error(f"Unexpected error while fetching trading volume for {symbol}: {e}")

    return 0.0  # Return 0.0 in case of any failure




def apply_ai_adjustment(base_min_qty: float, symbol: str) -> float:
    """
    Dynamically adjust the minimum trade quantity using AI techniques.

    Args:
        base_min_qty: The base minimum quantity from Binance.
        symbol: Trading pair symbol.

    Returns:
        float: Adjusted minimum quantity.
    """
    try:
        # Simulated AI-based dynamic adjustment logic
        logging.info(f"Applying AI adjustment for {symbol}. Base minimum quantity: {base_min_qty}")
        
        # Example AI logic: Retrieve historical trading data and predict volatility
        historical_trading_volume = get_historical_trading_volume(symbol)
        volatility_factor = predict_volatility(symbol)
        
        # Calculate the adjustment factor
        ai_adjustment_factor = (1 + volatility_factor) * (1 + historical_trading_volume / 1e6)  # Example scaling
        
        # Validate the adjustment factor to avoid extreme values
        if ai_adjustment_factor < 0.5 or ai_adjustment_factor > 2.0:  # Example bounds
            logging.warning(f"AI adjustment factor out of bounds: {ai_adjustment_factor}. Using default.")
            ai_adjustment_factor = 1.0  # Default to no adjustment
        
        # Compute the adjusted quantity
        adjusted_qty = base_min_qty * ai_adjustment_factor
        adjusted_qty = round(adjusted_qty, 8)  # Ensure precision
        
        # Log the results
        logging.info(f"AI-adjusted quantity for {symbol}: {adjusted_qty} (Factor: {ai_adjustment_factor})")
        
        # Ensure the adjusted quantity is not below the base minimum
        return max(base_min_qty, adjusted_qty)
    except Exception as e:
        logging.warning(f"AI adjustment failed for {symbol}: {e}")
        return base_min_qty  # Return the base minimum as fallback



def get_historical_trading_volume(symbol: str) -> float:
    """
    Fetch historical trading volume for a symbol (mock implementation for AI).

    Args:
        symbol: Trading pair symbol.

    Returns:
        float: Average trading volume over a recent period.
    """
    # Simulate retrieval of historical trading volume
    # Replace with actual Binance API calls or database queries
    simulated_volume = 50000.0  # Example: 50,000 units traded in the past period
    logging.info(f"Simulated historical trading volume for {symbol}: {simulated_volume}")
    return simulated_volume


import asyncio
import logging
import numpy as np
import pandas as pd
from binance import AsyncClient
from binance.exceptions import BinanceAPIException, BinanceRequestException
from arch import arch_model
import optuna
import shap

async def predict_volatility(client: AsyncClient, symbol: str, interval: str = '1d', lookback: str = '60 days ago UTC') -> float:
    """
    Predict volatility for a symbol based on historical data using an optimized GARCH(1,1) model and provide SHAP explanations.

    Args:
        client (AsyncClient): Binance AsyncClient instance.
        symbol (str): Trading pair symbol (e.g., "BTCUSDT").
        interval (str): Kline interval (default: '1d' for daily data).
        lookback (str): Time period to look back for historical data (default: '60 days ago UTC').

    Returns:
        float: Predicted volatility as a factor (e.g., 0.05 for 5%).
    """
    try:
        # Fetch historical klines
        klines = await client.get_historical_klines(symbol, interval, lookback)
        
        if not klines:
            logging.warning(f"No historical klines fetched for {symbol}.")
            return 0.0

        # Extract closing prices
        closing_prices = [float(kline[4]) for kline in klines]
        
        if len(closing_prices) < 30:
            logging.warning(f"Not enough closing price data to calculate volatility for {symbol}.")
            return 0.0

        # Calculate daily returns
        returns = 100 * np.diff(closing_prices) / closing_prices[:-1]
        returns = pd.Series(returns)

        # Define the objective function for Optuna
        def objective(trial):
            p = trial.suggest_int('p', 1, 5)
            q = trial.suggest_int('q', 1, 5)
            try:
                model = arch_model(returns, vol='Garch', p=p, q=q, dist='Normal')
                model_fit = model.fit(disp='off')
                # Use AIC as the optimization metric
                return model_fit.aic
            except Exception as e:
                # In case of failure, return a large AIC
                return float('inf')

        # Create and run the Optuna study
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=50)

        # Retrieve the best p and q
        best_p = study.best_params['p']
        best_q = study.best_params['q']
        logging.info(f"Optuna optimized GARCH parameters for {symbol}: p={best_p}, q={best_q}")

        # Fit the GARCH model with optimized parameters
        model = arch_model(returns, vol='Garch', p=best_p, q=best_q, dist='Normal')
        model_fit = model.fit(disp='off')

        # Forecast volatility for the next day
        forecast = model_fit.forecast(horizon=1)
        forecast_variance = forecast.variance.iloc[-1, 0]
        forecast_volatility = np.sqrt(forecast_variance) / 100  # Convert back to decimal

        logging.info(f"Predicted Volatility for {symbol}: {forecast_volatility:.4f} ({forecast_volatility*100:.2f}%)")

        # SHAP Explanation (Using a simple feature importance approach)
        # Since GARCH models are statistical, we'll create features based on lagged returns
        feature_window = 5  # Number of lagged returns to use as features
        if len(returns) < feature_window:
            logging.warning(f"Not enough data to generate features for SHAP explanation for {symbol}.")
            return forecast_volatility

        # Prepare the dataset for SHAP
        df_features = pd.DataFrame({
            f'return_lag_{i}': returns.shift(i) for i in range(1, feature_window + 1)
        }).dropna()
        df_target = returns.iloc[feature_window:]

        # Train a simple linear regression model for demonstration
        from sklearn.linear_model import LinearRegression
        model_lr = LinearRegression()
        model_lr.fit(df_features, df_target)

        # Initialize SHAP explainer
        explainer = shap.Explainer(model_lr, df_features)
        shap_values = explainer(df_features)

        # Visualize SHAP values (this requires a display; in headless environments, save to file)
        shap.summary_plot(shap_values, df_features, show=False)
        plt.savefig(f'shap_summary_{symbol}.png')
        plt.close()
        logging.info(f"SHAP summary plot saved as shap_summary_{symbol}.png")

        return forecast_volatility

    except BinanceAPIException as e:
        logging.error(f"Binance API Exception while predicting volatility for {symbol}: {e}")
    except BinanceRequestException as e:
        logging.error(f"Binance Request Exception while predicting volatility for {symbol}: {e}")
    except Exception as e:
        logging.error(f"Unexpected error while predicting volatility for {symbol}: {e}")

    return 0.0



def parse_order_error(error) -> str:
    """
    Parse Binance API error and return a user-friendly message.

    Args:
        error (Exception): The error object received from the Binance API.

    Returns:
        str: Parsed error message.
    """
    error_message = str(error)
    if "LOT_SIZE" in error_message:
        return "Order quantity does not meet the LOT_SIZE filter. Adjust the quantity to meet Binance's requirements."
    elif "PRICE_FILTER" in error_message:
        return "Price does not meet the PRICE_FILTER requirements. Ensure the price is within acceptable limits."
    elif "INSUFFICIENT_BALANCE" in error_message:
        return "Insufficient balance to place the order. Reduce the order quantity or check your balance."
    elif "APIERROR" in error_message:
        return "API error occurred. Review your API configuration and parameters."
    elif "EXCHANGE_UNAVAILABLE" in error_message:
        return "Binance exchange is temporarily unavailable. Try again later."
    else:
        return f"Unexpected error: {error_message}"



async def ai_trade_decision(client: AsyncClient, symbol: str, side: str, quantity: float, price: float) -> bool:
    """
    Make AI-driven trading decisions based on predictions and market sentiment.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.
        side (str): 'BUY' or 'SELL'.
        quantity (float): Quantity of the asset to trade.
        price (float): Current price of the asset.

    Returns:
        bool: True if the trade should proceed, False otherwise.
    """
    try:
        async with bot_data_lock:
            predicted_price = bot_data.get("predicted_price", None)
        if predicted_price is None:
            logging.warning("Predicted price is not available. Skipping trade decision.")
            return False

        # Fetch sentiment analysis
        sentiment = analyze_market_sentiment()
        if sentiment is None:
            sentiment = "Neutral"  # Default sentiment if unavailable

        # Predict volatility for the symbol
        volatility = predict_volatility(symbol)
        if volatility is None:
            logging.warning(f"Volatility prediction failed for {symbol}. Proceeding cautiously.")
            volatility = 0.0  # Default value if unavailable

        # Risk management logic
        if sentiment == "Extreme Greed" and side == "BUY":
            logging.warning("Market sentiment indicates Extreme Greed. Avoiding BUY order.")
            return False
        elif sentiment == "Extreme Fear" and side == "SELL":
            logging.warning("Market sentiment indicates Extreme Fear. Avoiding SELL order.")
            return False

        if volatility > CONFIG["MAX_VOLATILITY_THRESHOLD"] and side == "BUY":
            logging.warning(f"High volatility detected ({volatility:.2f}). Avoiding BUY order.")
            return False

        # Trading logic
        if side == "BUY" and predicted_price > price:
            logging.info(f"BUY decision for {symbol} at {price:.2f}, predicted: {predicted_price:.2f}.")
            return True
        elif side == "SELL" and predicted_price < price:
            logging.info(f"SELL decision for {symbol} at {price:.2f}, predicted: {predicted_price:.2f}.")
            return True
        else:
            logging.warning("Trade decision not favorable. Skipping trade.")
            return False

    except Exception as e:
        logging.error(f"Error in ai_trade_decision: {e}")
        return False



async def predict_price(client: AsyncClient, symbol: str) -> float:
    """
    Predict the future price of a given symbol using an Ensemble model.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.

    Returns:
        float: Predicted price or None if an error occurs.
    """
    try:
        # Use ensemble prediction
        predicted_price = ensemble_predict(symbol)
        
        # Update bot_data
        async with bot_data_lock:
            bot_data["predicted_price"] = predicted_price  # Update bot_data

        return predicted_price
    except Exception as e:
        logging.error(f"Error in predict_price for {symbol}: {e}")
        return None


def predict_volatility(symbol: str) -> float:
    """
    Predict volatility for a symbol based on historical data or model.
    Example: Use statistical methods to predict volatility.
    """
    # For demonstration, let's use a dummy function that randomly predicts volatility
    volatility = random.uniform(0.01, 0.1)  # Random volatility between 1% and 10%
    logging.info(f"Predicted Volatility for {symbol}: {volatility}")
    return volatility


async def send_alert(message: str) -> None:
    """
    Send alerts via email and Slack.

    Args:
        message (str): The alert message.

    Returns:
        None
    """
    await send_email_alert(message)
    await send_slack_alert(message)


async def send_email_alert(message: str) -> None:
    """
    Send an email alert.

    Args:
        message (str): The alert message.

    Returns:
        None
    """
    try:
        import smtplib
        from email.mime.text import MIMEText

        sender = CONFIG["ALERT_EMAIL"]
        receivers = [CONFIG["ALERT_EMAIL"]]
        msg = MIMEText(message)
        msg['Subject'] = 'Trading Bot Alert'
        msg['From'] = sender
        msg['To'] = ", ".join(receivers)

        # Setup the SMTP server (example using Gmail)
        smtp_server = "smtp.gmail.com"
        smtp_port = 587
        smtp_username = "raspberryrasppy@gmail.com"
        smtp_password = "snvb wnpd oymk rjks"

        server = smtplib.SMTP(smtp_server, smtp_port)
        server.starttls()
        server.login(smtp_username, smtp_password)
        server.sendmail(sender, receivers, msg.as_string())
        server.quit()

        logging.info("Email alert sent successfully.")
    except Exception as e:
        logging.error(f"Failed to send email alert: {e}")


async def send_slack_alert(message: str) -> None:
    """
    Send a Slack alert.

    Args:
        message (str): The alert message.

    Returns:
        None
    """
    try:
        slack_webhook_url = CONFIG["ALERT_SLACK_WEBHOOK"]
        if not slack_webhook_url:
            logging.warning("Slack webhook URL not configured.")
            return

        payload = {"text": message}
        async with aiohttp.ClientSession() as session:
            async with session.post(slack_webhook_url, json=payload) as resp:
                if resp.status == 200:
                    logging.info("Slack alert sent successfully.")
                else:
                    logging.error(f"Failed to send Slack alert. Status: {resp.status}")
    except Exception as e:
        logging.error(f"Failed to send Slack alert: {e}")


def retry_order(client: AsyncClient, symbol: str, side: str, quantity: float, price: float) -> None:
    """
    Retry the order placement if the first attempt failed, with adjusted parameters if necessary.
    """
    logging.info(f"Retrying {side} order for {symbol} at price {price}...")
    # Adjust parameters if needed (e.g., reduce quantity or change price by a small margin)
    adjusted_price = price * random.uniform(0.98, 1.02)  # Adjust price by a small factor
    adjusted_quantity = quantity * random.uniform(0.9, 1.1)  # Adjust quantity slightly
    asyncio.create_task(place_order(client, symbol, side, adjusted_quantity, adjusted_price))


import numpy as np
from sklearn.linear_model import LinearRegression

async def can_afford_trade(
    client: AsyncClient,
    quantity: float,
    price: float,
    buffer: float = 0.01,
    fee_rate: float = 0.001,
    asset: str = "USDT",
    min_trade_threshold: float = 0.0001,
) -> bool:
    """
    Check if the bot has enough balance to afford the trade, including a buffer for fees.
    Incorporates LOT_SIZE constraints to ensure trade quantity adheres to Binance requirements.

    Args:
        client: Binance AsyncClient instance.
        quantity (float): Quantity of the asset to trade.
        price (float): Price at which to trade.
        buffer (float): Percentage buffer to reserve (default: 1%).
        fee_rate (float): Trading fee rate (default: 0.1% or 0.001).
        asset (str): The asset to use for payment (default: USDT).
        min_trade_threshold (float): Minimum allowable trade amount.

    Returns:
        bool: True if affordable, False otherwise.
    """
    try:
        # Fetch LOT_SIZE constraints
        lot_size_filters = await fetch_lot_size_filters(client, asset + "USDT")
        min_qty = lot_size_filters.get("minQty", 0.001)
        step_size = lot_size_filters.get("stepSize", 0.0001)

        # Ensure quantity adheres to LOT_SIZE constraints
        quantity = max(min_qty, (quantity // step_size) * step_size)

        # Calculate total cost of the trade, including trading fees
        total_cost = quantity * price
        total_cost_with_fees = total_cost * (1 + fee_rate)

        # Fetch balance of the base asset
        balance = await get_balance(client, asset)

        # Log balances and cost for debugging
        if balance is None:
            logging.error(f"Failed to fetch {asset} balance. Cannot proceed with trade.")
            return False

        logging.info(f"Total cost of trade (including fees): {total_cost_with_fees:.2f} {asset}")
        logging.info(f"Available {asset} balance: {balance:.2f}")

        # Reserve buffer and check affordability
        total_cost_with_buffer = total_cost_with_fees * (1 + buffer)

        if balance >= total_cost_with_buffer:
            logging.info(f"Sufficient {asset} balance to afford the trade with buffer and fees.")
            return True

        # Log insufficient balance
        logging.warning(f"Insufficient {asset} balance. Required: {total_cost_with_buffer:.2f}, Available: {balance:.2f}")

        # Dynamically adjust the quantity to fit the available balance
        logging.info("Dynamically adjusting trade quantity to fit available balance.")
        while quantity > min_trade_threshold:
            quantity *= 0.95  # Reduce quantity by 5%
            quantity = max(min_qty, (quantity // step_size) * step_size)  # Adhere to step size
            total_cost = quantity * price
            total_cost_with_fees = total_cost * (1 + fee_rate)
            total_cost_with_buffer = total_cost_with_fees * (1 + buffer)

            logging.info(f"Trying adjusted quantity: {quantity:.5f}, Total cost: {total_cost_with_buffer:.2f}")

            if balance >= total_cost_with_buffer:
                logging.info(f"Adjusted quantity is affordable. Proceeding with {quantity:.5f}.")
                return True

        logging.warning(f"Even the smallest quantity {quantity:.5f} is not affordable. Skipping trade.")
        return False

    except Exception as e:
        logging.error(f"Error in can_afford_trade for {asset}: {e}")
        return False

async def evaluate_and_trade(client: AsyncClient, symbol: str = "BTCUSDT") -> None:
    """
    Continuously evaluate market conditions and execute trades.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair to evaluate and trade.

    Returns:
        None
    """
    global circuit_breaker_triggered

    # Ensure circuit breaker threshold is initialized
    if symbol not in CONFIG["CIRCUIT_BREAKER_THRESHOLD"]:
        CONFIG["CIRCUIT_BREAKER_THRESHOLD"][symbol] = await calculate_circuit_breaker_threshold(client, symbol)

    while not circuit_breaker_triggered:
        try:
            # Fetch the latest price from the latest_price dictionary
            async with price_lock:
                price = latest_price.get(symbol, None)
            if price is None:
                logging.info(f"No price data for {symbol} yet. Retrying...")
                await asyncio.sleep(1)  # Short wait to prevent spamming
                continue

            # Monitor system and market conditions
            await monitor_whale_trades(client, symbol)  # Await the async function
            await fetch_fear_greed_index()  # Fetch and update the Fear and Greed Index
            await monitor_system_health()  # Properly await health check

            # Update current balance
            await update_current_balance(client, asset="USDT")

            # Log current metrics
            async with bot_data_lock:
                health_data = {
                    "CPU": bot_data.get("CPU", "N/A"),
                    "Memory": bot_data.get("Memory", "N/A"),
                    "Disk": bot_data.get("Disk", "N/A")
                }
                bot_data["logs"].append(f"Current Price: {price:.2f}, Health: CPU={health_data['CPU']}%, Memory={health_data['Memory']}%, Disk={health_data['Disk']}%")
            logging.info(f"Current Price: {price:.2f}, Health: CPU={health_data['CPU']}%, Memory={health_data['Memory']}%, Disk={health_data['Disk']}%")

            # Implement circuit breaker (dynamic threshold)
            threshold = CONFIG["CIRCUIT_BREAKER_THRESHOLD"].get(symbol, float('inf'))  # Default to no circuit breaker
            if price <= threshold:
                circuit_breaker_triggered = True
                async with bot_data_lock:
                    bot_data["logs"].append(f"âš ï¸ Circuit Breaker Triggered! {symbol} price dropped to {price:.2f}")
                logging.warning(f"Circuit Breaker Triggered! {symbol} price dropped to {price:.2f}")
                await send_alert(f"Circuit Breaker Triggered! {symbol} price dropped to {price:.2f} USDT")
                break

            # Predict the next price
            predicted_price = await predict_price(client, symbol)
            if predicted_price is None:
                logging.warning(f"Could not predict price for {symbol}. Skipping trade.")
                await asyncio.sleep(CONFIG["SLEEP_INTERVAL"])
                continue

            # Calculate dynamic trade quantity based on available balance
            usdt_balance = await get_balance(client, "USDT")
            if usdt_balance and usdt_balance > 0:
                trade_quantity = usdt_balance / price  # Calculate quantity
                # Fetch LOT_SIZE constraints
                lot_size_filters = await fetch_lot_size_filters(client, symbol)
                min_qty = lot_size_filters.get("minQty", 0.001)
                step_size = lot_size_filters.get("stepSize", 0.0001)

                # Adjust quantity to adhere to LOT_SIZE constraints
                trade_quantity = max(min_qty, (trade_quantity // step_size) * step_size)

                if trade_quantity < min_qty:
                    logging.warning(f"Calculated trade quantity {trade_quantity} is below minimum allowed: {min_qty}. Skipping trade.")
                    await asyncio.sleep(CONFIG["SLEEP_INTERVAL"])
                    continue
            else:
                logging.warning("Insufficient balance. Skipping trade.")
                await asyncio.sleep(CONFIG["SLEEP_INTERVAL"])
                continue

            # Trade based on prediction and strategy
            if price < predicted_price:
                await place_order(client, symbol, "BUY", quantity=trade_quantity, price=price)
            elif price > predicted_price:
                await place_order(client, symbol, "SELL", quantity=trade_quantity, price=price)

            # Include sentiment and market analysis
            sentiment = analyze_market_sentiment()
            if sentiment == "Extreme Greed" and price > predicted_price:
                logging.warning("Avoiding SELL due to Extreme Greed sentiment.")
            elif sentiment == "Extreme Fear" and price < predicted_price:
                logging.warning("Avoiding BUY due to Extreme Fear sentiment.")

            # Calculate Value at Risk (VaR)
            portfolio_value = bot_data.get("current_balance_value", 0.0)
            var = calculate_value_at_risk(portfolio_value, CONFIG["VAR_CONFIDENCE_LEVEL"])
            async with bot_data_lock:
                bot_data["risk"] = var
            logging.info(f"Value at Risk (VaR) at {CONFIG['VAR_CONFIDENCE_LEVEL']*100}% confidence: {var:.2f} USDT")

            # Adjust trading strategy based on VaR
            if var > portfolio_value * 0.05:  # Example threshold
                logging.warning("High VaR detected. Adjusting trading strategy.")
                await send_alert(f"High VaR detected: {var:.2f} USDT. Adjusting trading strategy.")

            # Sleep to prevent API spamming
            await asyncio.sleep(CONFIG["SLEEP_INTERVAL"])

        except Exception as e:
            # Handle rate limits and other errors
            if "429" in str(e):
                logging.warning("Rate limit reached. Sleeping for 10 seconds...")
                await asyncio.sleep(10)  # Backoff for rate limits
            else:
                logging.error(f"Error in evaluate_and_trade for {symbol}: {e}")
                async with bot_data_lock:
                    bot_data["logs"].append(f"Error: {str(e)}")
                await send_alert(f"Error in evaluate_and_trade: {e}")
                await asyncio.sleep(CONFIG["SLEEP_INTERVAL"])  # Retry after short pause




async def get_last_60_prices(client: AsyncClient, symbol: str, interval: str = Client.KLINE_INTERVAL_5MINUTE) -> List[float]:
    """
    Fetch the last 60 closing prices for a given symbol, dynamically extending the time range
    and applying additional logics like retrying and predictive fallback when necessary.

    Args:
        client: Binance AsyncClient instance.
        symbol (str): Trading pair symbol.
        interval (str): Kline interval.

    Returns:
        list: List of last 60 closing prices.
    """
    time_ranges = [
        "3 hours ago UTC",
        "6 hours ago UTC",
        "12 hours ago UTC",
        "1 day ago UTC",
        "3 days ago UTC",
        "7 days ago UTC",
        "15 days ago UTC"
    ]
    closing_prices = []

    for time_range in time_ranges:
        try:
            # Fetch historical data from Binance API
            klines = await safe_api_call(client.get_historical_klines, symbol, interval, time_range)

            if not klines:
                logging.warning(f"No klines fetched for {symbol} in range {time_range}.")
                continue

            # Extract closing prices and add them to the list
            for kline in klines:
                close_price = float(kline[4])  # Closing price
                closing_prices.append(close_price)

            # Ensure we have at least 60 prices
            if len(closing_prices) >= 60:
                return closing_prices[-60:]  # Return the last 60 closing prices
            else:
                logging.warning(f"Only {len(closing_prices)} prices found for {symbol} in range {time_range}. Retrying...")

        except Exception as e:
            logging.error(f"Error fetching prices for {symbol} with range {time_range}: {e}")

    # Log if insufficient data after all attempts
    if len(closing_prices) < 60:
        logging.error(f"Failed to fetch sufficient data for {symbol}. Found only {len(closing_prices)} prices.")

        # Apply fallback logic with machine learning prediction
        if closing_prices:
            predicted_prices = apply_fallback_prediction(closing_prices)
            logging.warning("Using fallback prediction to supplement insufficient data.")
            closing_prices.extend(predicted_prices)

            # Trim to the last 60 prices if extended data exceeds the requirement
            return closing_prices[-60:]

    return closing_prices[-60:]  # Return available prices, even if fewer than 60

def apply_fallback_prediction(closing_prices: List[float], predict_steps: int = 60) -> List[float]:
    """
    Generate fallback predictions for insufficient data using a combination of linear and polynomial regression models.

    Args:
        closing_prices (list): List of available closing prices.
        predict_steps (int): Number of future steps to predict.

    Returns:
        list: List of predicted prices to supplement the available data.
    """
    try:
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import PolynomialFeatures
        import numpy as np

        if len(closing_prices) < 5:
            logging.warning(
                f"Insufficient data for meaningful prediction. Received only {len(closing_prices)} prices."
            )
            return [closing_prices[-1]] * predict_steps if closing_prices else []

        logging.info(f"Applying fallback prediction on {len(closing_prices)} closing prices.")

        # Prepare data for regression
        X = np.arange(len(closing_prices)).reshape(-1, 1)
        y = np.array(closing_prices)

        # Train a simple linear regression model
        logging.info("Training Linear Regression model...")
        linear_model = LinearRegression()
        linear_model.fit(X, y)

        # Predict using linear regression
        future_X = np.arange(len(closing_prices), len(closing_prices) + predict_steps).reshape(-1, 1)
        linear_predictions = linear_model.predict(future_X)
        logging.info(f"Linear regression predicted values: {linear_predictions[:5]}...")

        # Train a polynomial regression model (degree=2 for quadratic trends)
        logging.info("Training Polynomial Regression model (degree=2)...")
        poly_features = PolynomialFeatures(degree=2)
        X_poly = poly_features.fit_transform(X)
        poly_model = LinearRegression()
        poly_model.fit(X_poly, y)

        # Predict using polynomial regression
        future_X_poly = poly_features.transform(future_X)
        poly_predictions = poly_model.predict(future_X_poly)
        logging.info(f"Polynomial regression predicted values: {poly_predictions[:5]}...")

        # Combine predictions (weighted average for flexibility)
        logging.info("Combining predictions from Linear and Polynomial models.")
        combined_predictions = 0.6 * linear_predictions + 0.4 * poly_predictions
        logging.info(f"Combined predicted values: {combined_predictions[:5]}...")

        # Logging reasoning behind predictions
        linear_trend = linear_model.coef_[0]
        poly_trend = np.polyfit(X.flatten(), y, deg=2)[0]  # Coefficient for the quadratic term

        if abs(linear_trend) > 0.1:
            logging.info(f"Linear model detected a significant trend: {linear_trend:.5f}.")
        else:
            logging.info("Linear model trend is weak; relying more on polynomial predictions.")

        if poly_trend > 0:
            logging.info(f"Polynomial model suggests a bullish trend (quadratic term: {poly_trend:.5f}).")
        elif poly_trend < 0:
            logging.info(f"Polynomial model suggests a bearish trend (quadratic term: {poly_trend:.5f}).")
        else:
            logging.info("Polynomial model suggests no strong directional bias.")

        logging.info("Advanced fallback predictions generated successfully using linear and polynomial regression.")
        return combined_predictions.tolist()

    except Exception as e:
        logging.error(f"Error in apply_fallback_prediction: {e}")
        return []


def backtest_strategy(symbol: str, historical_prices: List[float]) -> float:
    """
    Backtest the trading strategy on historical data.

    Args:
        symbol (str): Trading pair symbol.
        historical_prices (list): List of historical closing prices.

    Returns:
        float: Total profit or loss from backtesting.
    """
    try:
        if len(historical_prices) < 60:
            logging.warning("Insufficient historical data for backtesting.")
            return 0.0

        # Example simple moving average crossover strategy
        df = pd.DataFrame(historical_prices, columns=['Close'])
        df['SMA_20'] = df['Close'].rolling(window=20).mean()
        df['SMA_50'] = df['Close'].rolling(window=50).mean()
        df.dropna(inplace=True)

        position = 0  # 0: No position, 1: Long position
        buy_price = 0.0
        profit = 0.0

        for index, row in df.iterrows():
            if row['SMA_20'] > row['SMA_50'] and position == 0:
                position = 1
                buy_price = row['Close']
                logging.info(f"Backtest BUY at {buy_price}")
            elif row['SMA_20'] < row['SMA_50'] and position == 1:
                position = 0
                sell_price = row['Close']
                trade_profit = sell_price - buy_price
                profit += trade_profit
                logging.info(f"Backtest SELL at {sell_price}, Profit: {trade_profit}")

        # If position is still open at the end
        if position == 1:
            sell_price = df.iloc[-1]['Close']
            trade_profit = sell_price - buy_price
            profit += trade_profit
            logging.info(f"Backtest SELL at {sell_price}, Profit: {trade_profit}")

        logging.info(f"Total Backtest Profit/Loss: {profit:.2f} USDT")
        return profit

    except Exception as e:
        logging.error(f"Error during backtesting: {e}")
        return 0.0



async def monitor_system_health() -> None:
    """
    Asynchronously monitor and log system health metrics with anomaly detection.
    Includes additional logging and data integration for improved diagnostics.
    """
    try:
        # Gather system health metrics
        cpu = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory().percent
        disk = psutil.disk_usage('/').percent

        # `get_network_latency` should remain awaited as it is async
        network_latency = await get_network_latency()

        # `get_io_usage` is a synchronous function and should not be awaited
        io_usage = get_io_usage()

        # Append metrics to their respective history lists
        cpu_history.append(cpu)
        memory_history.append(memory)
        disk_usage_history.append(disk)

        # Limit history to the last 60 entries
        cpu_history[:] = cpu_history[-60:]
        memory_history[:] = memory_history[-60:]
        disk_usage_history[:] = disk_usage_history[-60:]

        # Prepare metrics dictionary
        metrics = {
            "CPU": cpu,
            "Memory": memory,
            "Disk": disk,
            "Network Latency": network_latency,
            "I/O Usage": io_usage,
        }

        # Log metrics and update bot data
        logging.info(
            f"System Metrics - CPU: {cpu}%, Memory: {memory}%, Disk: {disk}%, "
            f"Network Latency: {network_latency}ms, I/O: {io_usage}"
        )
        async with bot_data_lock:
            bot_data["CPU"] = cpu
            bot_data["Memory"] = memory
            bot_data["Disk"] = disk
            bot_data["Network Latency"] = network_latency
            bot_data["I/O Usage"] = io_usage
            bot_data["logs"].append(
                f"System Health - CPU: {cpu}%, Memory: {memory}%, Disk: {disk}%, "
                f"Network Latency: {network_latency}ms, I/O: {io_usage}"
            )

        # Detect anomalies using the anomaly detection function
        if detect_anomaly(metrics):
            anomaly_details = []
            if cpu > 98:
                anomaly_details.append("High CPU Usage")
            if memory > 98:
                anomaly_details.append("High Memory Usage")
            if disk > 98:
                anomaly_details.append("High Disk Usage")
            if network_latency > 200:
                anomaly_details.append(f"High Network Latency ({network_latency}ms)")
            if io_usage.get("read_bytes", 0) > 90:  # Fixed key usage for `read_bytes`
                anomaly_details.append(f"High I/O Read Usage ({io_usage['read_bytes']}%)")
            if io_usage.get("write_bytes", 0) > 90:  # Fixed key usage for `write_bytes`
                anomaly_details.append(f"High I/O Write Usage ({io_usage['write_bytes']}%)")

            # Prepare and log anomaly message
            anomaly_message = (
                "ðŸš¨ System anomaly detected: "
                + (", ".join(anomaly_details) if anomaly_details else "Unknown Anomaly")
                + "! Initiating protective measures."
            )
            logging.warning(anomaly_message)
            async with bot_data_lock:
                bot_data["logs"].append(anomaly_message)

            # Trigger protective measures by pausing trading
            asyncio.create_task(pause_trading())

    except Exception as e:
        # Log unexpected errors
        logging.error(f"Error monitoring system health: {e}")
        async with bot_data_lock:
            bot_data["logs"].append(f"Error monitoring system health: {e}")



async def pause_trading() -> None:
    """
    Pause all trading activities in response to system anomalies.
    """
    try:
        # Update the bot's status to 'Paused'
        async with bot_data_lock:
            bot_data["status"] = "Paused due to system anomaly"
            bot_data["logs"].append("Trading activities have been paused due to a system anomaly.")

        logging.info("Trading activities have been paused due to a system anomaly.")

        # Send an alert about the pause
        await send_alert("Trading activities have been paused due to a system anomaly.")

        # Optional: Implement additional protective measures
        # For example, cancel active trading tasks, close positions, etc.
        # Example:
        # await close_all_positions(client)

    except Exception as e:
        logging.error(f"Error in pause_trading: {e}")
        # Optionally, send an alert about the failure to pause trading
        await send_alert(f"Error in pause_trading: {e}")



# Self-Healing Features

async def self_healing_watchdog():
    """
    Watchdog that monitors critical tasks and restarts them if they fail.
    """
    while True:
        try:
            # Check if critical tasks are still running
            # Example: Ensure that evaluate_and_trade task is running
            # This requires tracking tasks, which can be implemented as needed
            await asyncio.sleep(60)  # Check every minute
        except Exception as e:
            logging.error(f"Watchdog encountered an error: {e}")
            await send_alert(f"Watchdog encountered an error: {e}")
            # Attempt to restart critical tasks or take other protective measures


async def send_whale_trade_alert(trade: Dict) -> None:
    """
    Send an alert when a whale trade is detected.

    Args:
        trade (dict): The trade data.

    Returns:
        None
    """
    try:
        message = f"Whale trade detected: {trade}"
        await send_alert(message)
    except Exception as e:
        logging.error(f"Failed to send whale trade alert: {e}")



# Self-Healing: Restarting Failed Tasks

async def restart_failed_tasks():
    """
    Monitor tasks and restart them if they have failed.
    """
    while True:
        try:
            # Implement logic to monitor tasks
            # This can be done by tracking task statuses or using callbacks
            await asyncio.sleep(30)  # Check every 30 seconds
        except Exception as e:
            logging.error(f"Error in restart_failed_tasks: {e}")
            await send_alert(f"Error in restart_failed_tasks: {e}")


# Updating ensemble_predict to handle missing models gracefully

def ensemble_predict(symbol: str) -> float:
    """
    Combine predictions from LSTM, GRU, and ARIMA models to produce a robust ensemble prediction
    using adaptive weighting, uncertainty quantification, and performance tracking.

    Args:
        symbol (str): Trading pair symbol.

    Returns:
        float: Ensemble predicted price.
    """
    try:
        # Storage for predictions and uncertainties
        predictions = {}
        uncertainties = {}
        weights = {"LSTM": 0.4, "GRU": 0.4, "ARIMA": 0.2}  # Initial weights
        historical_prices = load_historical_data(symbol)

        # Ensure sufficient data is available
        if len(historical_prices) < 60:
            logging.warning(f"Insufficient historical data for ensemble prediction. Received only {len(historical_prices)} prices.")
            return 0.0

        # Normalize data
        scaled_data = scaler.transform(np.array(historical_prices[-60:]).reshape(-1, 1))
        input_data = scaled_data.reshape(1, 60, 1)

        # Step 1: LSTM Prediction
        if model_lstm:
            try:
                lstm_pred = model_lstm.predict(input_data, verbose=0)[0][0]
                lstm_uncertainty = np.std(model_lstm.predict(input_data, verbose=0).flatten())  # Simulated uncertainty
                predictions["LSTM"] = lstm_pred
                uncertainties["LSTM"] = lstm_uncertainty
                logging.info(f"LSTM Prediction: {lstm_pred:.2f}, Uncertainty: {lstm_uncertainty:.4f}")
            except Exception as e:
                logging.error(f"LSTM prediction error: {e}")
        else:
            logging.warning("LSTM model is not loaded.")

        # Step 2: GRU Prediction
        if model_gru:
            try:
                gru_pred = model_gru.predict(input_data, verbose=0)[0][0]
                gru_uncertainty = np.std(model_gru.predict(input_data, verbose=0).flatten())  # Simulated uncertainty
                predictions["GRU"] = gru_pred
                uncertainties["GRU"] = gru_uncertainty
                logging.info(f"GRU Prediction: {gru_pred:.2f}, Uncertainty: {gru_uncertainty:.4f}")
            except Exception as e:
                logging.error(f"GRU prediction error: {e}")
        else:
            logging.warning("GRU model is not loaded.")

        # Step 3: ARIMA Prediction
        if model_arima:
            try:
                arima_pred = model_arima.forecast(steps=1)[0]
                arima_uncertainty = model_arima.forecast(steps=1, alpha=0.05)[1][0][1] - arima_pred  # Using confidence intervals
                predictions["ARIMA"] = arima_pred
                uncertainties["ARIMA"] = arima_uncertainty
                logging.info(f"ARIMA Prediction: {arima_pred:.2f}, Uncertainty: {arima_uncertainty:.4f}")
            except Exception as e:
                logging.error(f"ARIMA prediction error: {e}")
        else:
            logging.warning("ARIMA model is not loaded.")

        # Step 4: Adaptive Weight Adjustment
        valid_predictions = {model: predictions[model] for model in predictions if model in weights}
        valid_weights = {model: weights[model] for model in valid_predictions}
        valid_uncertainties = {model: uncertainties.get(model, 1.0) for model in valid_predictions}

        # Adjust weights inversely proportional to uncertainty
        for model in valid_weights:
            valid_weights[model] /= valid_uncertainties[model]
        total_weight = sum(valid_weights.values())
        normalized_weights = {model: weight / total_weight for model, weight in valid_weights.items()}

        logging.info(f"Normalized Weights: {normalized_weights}")

        # Step 5: Compute Ensemble Prediction
        ensemble_pred = sum(valid_predictions[model] * normalized_weights[model] for model in valid_predictions)
        logging.info(f"Ensemble Prediction for {symbol}: {ensemble_pred:.2f}")

        # Logging contributions
        for model, prediction in valid_predictions.items():
            logging.info(f"{model} contribution: Prediction={prediction:.2f}, Weight={normalized_weights[model]:.2f}, Uncertainty={valid_uncertainties[model]:.4f}")

        # Performance Tracking and Self-Tuning
        update_model_performance(symbol, predictions, uncertainties)

        return ensemble_pred

    except Exception as e:
        logging.critical(f"Critical error in ensemble_predict: {e}", exc_info=True)
        return 0.0


def update_model_performance(symbol: str, predictions: dict, uncertainties: dict) -> None:
    """
    Track and update the performance of individual models over time to improve weights dynamically.

    Args:
        symbol (str): Trading pair symbol.
        predictions (dict): Dictionary of model predictions.
        uncertainties (dict): Dictionary of model uncertainties.

    Returns:
        None
    """
    try:
        # Historical performance tracking logic
        performance_data = load_historical_model_performance(symbol)

        for model in predictions:
            pred_error = abs(predictions[model] - latest_price[symbol])  # Compare with actual price
            model_performance = 1 / (1 + pred_error + uncertainties.get(model, 1.0))  # Lower error = higher performance
            performance_data[model].append(model_performance)

            # Maintain a rolling performance score
            if len(performance_data[model]) > 100:
                performance_data[model].pop(0)

            logging.info(f"Updated {model} performance: {model_performance:.4f} (Rolling Avg: {np.mean(performance_data[model]):.4f})")

        save_historical_model_performance(symbol, performance_data)

    except Exception as e:
        logging.error(f"Error updating model performance: {e}")


def load_historical_model_performance(symbol: str) -> dict:
    """
    Load the historical performance of models for the given symbol.

    Args:
        symbol (str): Trading pair symbol.

    Returns:
        dict: Historical performance data for each model.
    """
    # Mock implementation: Replace with actual file/database loading logic
    performance_file = f"{symbol}_model_performance.pkl"
    if os.path.exists(performance_file):
        with open(performance_file, "rb") as file:
            return pickle.load(file)
    return {"LSTM": [], "GRU": [], "ARIMA": []}


def save_historical_model_performance(symbol: str, performance_data: dict) -> None:
    """
    Save the historical performance of models for the given symbol.

    Args:
        symbol (str): Trading pair symbol.
        performance_data (dict): Performance data to save.

    Returns:
        None
    """
    # Mock implementation: Replace with actual file/database saving logic
    performance_file = f"{symbol}_model_performance.pkl"
    with open(performance_file, "wb") as file:
        pickle.dump(performance_data, file)



# Self-Healing: Automatically Reloading Models if Prediction Fails

def safe_predict(model, input_data):
    """
    Safely perform a prediction with the given model.

    Args:
        model: The machine learning model.
        input_data: The input data for prediction.

    Returns:
        float: The prediction result or 0.0 if prediction fails.
    """
    try:
        prediction = model.predict(input_data, verbose=0)[0][0]
        return prediction
    except Exception as e:
        logging.error(f"Prediction failed: {e}")
        return 0.0


# Self-Healing: Automatically Reload Models if They Fail

async def model_health_check():
    """
    Periodically check the health of the models and reload/retrain them if necessary.
    """
    while True:
        try:
            # Check if LSTM model is loaded
            if not model_lstm:
                logging.warning("LSTM model not loaded. Reloading or Retraining...")
                try:
                    await load_lstm_model(client)
                except Exception as e:
                    logging.error(f"Error reloading LSTM model: {e}. Retraining...")
                    await retrain_and_save_model(client, symbol="ETHUSDT", model_path="models/model_lstm.h5")
            
            # Check if GRU model is loaded
            if not model_gru:
                logging.warning("GRU model not loaded. Reloading or Retraining...")
                try:
                    await load_gru_model(client)
                except Exception as e:
                    logging.error(f"Error reloading GRU model: {e}. Retraining...")
                    await retrain_gru_model(client, symbol="ETHUSDT", model_path="models/model_gru.h5")
            
            # Check if ARIMA model is loaded
            if not model_arima:
                logging.warning("ARIMA model not loaded. Reloading or Retraining...")
                try:
                    await load_arima_model(client)
                except Exception as e:
                    logging.error(f"Error reloading ARIMA model: {e}. Retraining...")
                    await retrain_arima_model(client, symbol="ETHUSDT", model_path="models/model_arima.pkl")

            # Sleep before the next health check
            await asyncio.sleep(3600)  # Check every hour

        except Exception as e:
            logging.error(f"Error in model_health_check: {e}")
            await send_alert(f"Error in model_health_check: {e}")
            await asyncio.sleep(600)  # Retry after 10 minutes if thereâ€™s an error


async def start_websocket(client: AsyncClient):
    """
    Start the Binance WebSocket to receive real-time ticker updates.
    """
    try:
        bsm = BinanceSocketManager(client)
        # Subscribe to the symbol ticker stream
        ts = bsm.symbol_ticker_socket('ETHUSDT')  # Modify as needed for other symbols
    
        async with ts as tscm:
            while True:
                res = await tscm.recv()
                if res and 'c' in res:
                    async with price_lock:
                        latest_price['ETHUSDT'] = float(res['c'])
                    logging.debug(f"WebSocket Updated ETHUSDT price: {latest_price['ETHUSDT']}")
    except Exception as e:
        logging.error(f"Error in start_websocket: {e}")
        await send_alert(f"WebSocket connection error: {e}")

# Update main_execution to include model_health_check

async def main_execution() -> None:
    global client
    client = await AsyncClient.create(api_key, api_secret, tld="us")
    tasks = []

    try:
        logging.info("ðŸ” Starting system monitoring...")
        tasks.append(asyncio.create_task(run_monitoring(client)))

        logging.info("ðŸš€ Starting WebSocket stream...")
        tasks.append(asyncio.create_task(start_websocket(client)))

        try:
            logging.info("ðŸ” Loading or training LSTM model...")
            await load_lstm_model(client)

            logging.info("ðŸ” Loading or training GRU model...")
            await load_gru_model(client)

            logging.info("ðŸ” Loading or training ARIMA model...")
            await load_arima_model(client)
        except Exception as e:
            logging.error(f"Error loading models: {e}")
            await send_alert(f"Error loading models: {e}")
            raise

        logging.info("ðŸ’° Fetching initial balances...")
        initial_balance = await get_balance(client, asset="USDT")
        async with bot_data_lock:
            bot_data["initial_balance"] = initial_balance
            bot_data["current_balance_value"] = initial_balance

        logging.info("ðŸ” Starting system monitoring...")
        tasks.append(asyncio.create_task(run_monitoring(client)))

        best_symbol = await get_best_trading_symbol(client)
        logging.info(f"ðŸ“ˆ Best trading symbol determined: {best_symbol}")

        logging.info("ðŸ“Š Starting evaluate and trade task...")
        tasks.append(asyncio.create_task(evaluate_and_trade(client, symbol=best_symbol)))

        logging.info("ðŸ› ï¸ Scheduling automated model retraining...")
        tasks.append(asyncio.create_task(schedule_model_retraining(client, symbol=best_symbol)))

        logging.info("ðŸ“‰ Starting backtesting task...")
        tasks.append(asyncio.create_task(run_backtesting(client, best_symbol)))

        logging.info("ðŸ›¡ï¸ Starting self-healing watchdog...")
        tasks.append(asyncio.create_task(self_healing_watchdog()))

        logging.info("ðŸ›¡ï¸ Starting model health check...")
        tasks.append(asyncio.create_task(model_health_check()))

        logging.info("âš™ï¸ Starting dynamic circuit breaker threshold updates...")
        tasks.append(asyncio.create_task(update_dynamic_thresholds(client)))

        logging.info("ðŸ“Š Starting live table display...")
        tasks.append(asyncio.create_task(display_live_table()))

        await asyncio.gather(*tasks, return_exceptions=True)

    except asyncio.CancelledError:
        logging.info("â¹ï¸ Tasks canceled. Cleaning up...")
    except Exception as e:
        logging.error(f"ðŸ”¥ An unexpected error occurred: {e}")
        await send_alert(f"An unexpected error occurred: {e}")
    finally:
        logging.info("ðŸ›‘ Shutting down tasks...")
        for task in tasks:
            try:
                task.cancel()
                await task
            except asyncio.CancelledError:
                logging.info("Task canceled successfully.")
            except Exception as task_exception:
                logging.error(f"Error canceling task: {task_exception}")

        if client:
            await client.close_connection()
            logging.info("âœ… Client connection closed.")

if __name__ == "__main__":
    try:
        asyncio.run(main_execution())
    except KeyboardInterrupt:
        logging.info("Bot stopped manually.")
    except Exception as e:
        logging.critical(f"Bot terminated unexpectedly: {e}")
